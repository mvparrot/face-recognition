---
title: "BEX3012 Project Report \n Detecting Facial Expressions in Professional Tennis Matches"
author: "Stephanie Kobakian"
date: "28 March 2017"
output: bookdown::pdf_document2
fig_caption: yes
out_width: 7
fontsize: 11pt
link-citations: true
---


```{r cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache=TRUE)

library(tidyverse)
library(dplyr)
library(readr)
library(GGally)
library(magick)
library(grid)
library(gridExtra)
library(ggplot2)

#Read in data files
ME <- read_csv("data/MicrosoftEmotions.csv")
SB <- read_csv("data/SkybiometryEmotions.csv")
GE <- read_csv("data/GoogleFacesEmotions.csv")
GF <- read_csv("data/GoogleFaces.csv")
DE <- read_csv("data/DominantEmotion.csv")
# 
# MEP <- read_csv("data/MicrosoftEmotions.csv")
# SBP <- read_csv("data/SkybiometryEmotions.csv")
# GEP <- read_csv("data/GoogleFacesEmotions.csv")

EmotionSoftwareSpecs <- read_csv("Figures/EmotionSoftwareSpecifications.csv")
```


```{r normalise, cache=FALSE}
#Create normalised SB values
SB <- SB %>%
  rowwise %>% mutate(z = (anger.confidence+
               disgust.confidence+
               fear.confidence+
               happiness.confidence+
               neutral_mood.confidence+
               sadness.confidence+
               surprise.confidence)) %>%
  mutate(anger = anger.confidence/z,
         disgust = disgust.confidence/z,
         fear = fear.confidence/z,
         happiness = happiness.confidence/z,
         neutral = neutral_mood.confidence/z,
         sadness = sadness.confidence/z,
         surprise = surprise.confidence/z)

#Create normalised GE values
Conf <- function(Emot){
ifelse(Emot=="VERY_UNLIKELY", 10,  
        ifelse(Emot == "UNLIKELY", 30, 
                ifelse(Emot == "POSSIBLE", 50, 
                ifelse(Emot == "LIKELY", 70, 
                ifelse(Emot == "VERY_LIKELY", 90, NA)))))
}
```

```{r mutate}
#confidence columns
GE <- GE %>% mutate(joyConf = Conf(joyLikelihood)) %>%
  mutate(sorrowConf = Conf(sorrowLikelihood)) %>%
  mutate(angerConf = Conf(angerLikelihood)) %>%
  mutate(surpriseConf = Conf(surpriseLikelihood))

#proportion columns
GE <- GE %>%
  rowwise %>% 
  mutate(neutralConf = 100 - (pmax(joyConf, sorrowConf, angerConf, surpriseConf))) %>%
  mutate(z = (joyConf+
                sorrowConf+
                angerConf+
                surpriseConf+
                neutralConf)) %>%
  mutate(anger = angerConf/z,
         joy = joyConf/z,
         neutral = neutralConf/z,
         sorrow = sorrowConf/z,
         surprise = surpriseConf/z)

#player name column
GE <- GE %>% rowwise %>%
  mutate(playerName = ifelse(PlayerNumber == 1, player1,
                                            ifelse(PlayerNumber == 2, player2,
                                                   ifelse(is.na(PlayerNumber), NA, NA))))
```

```{r join}
ME <- left_join(ME, GF, by = "FileName") 
SB <- left_join(SB, GF, by = c("aname"="FileName")) 
```

```{r playerSubset}
library(dplyr)
GEP <- GE %>% dplyr::filter(detect == "Player")
MEP <- ME %>% dplyr::filter(detect == "Player")
SBP <- SB %>% dplyr::filter(detect == "Player")
```

```{r NeutralProportions}
S4 <- select(SBP, FileName = aname, Skybiometry = neutral)
S5 <- select(MEP, FileName, Microsoft = neutral)
S6 <- select(GEP, FileName, Google = neutral)
J3 <- full_join(S4, S5, by = "FileName")
J4 <- full_join(J3, S6, by = "FileName")
```

\newpage
# Introduction

Facial Recognition is beginning to be explored in sports environments, this presents quality issues.


helpful for applications


\newpage

# Materials

## Images

The image set contained 6406 still shot 800x450px size video frames, taken every 3 seconds from 5 minute segments sampled from 105 Australian Open 2016 match broadcast videos.
The broadcast videos are a combination of multiple camera feeds, these angles vary depending on the court the match was played on.

```{r ExampleImage, echo=F, fig.cap = "Example Image: 2016_CT6_R01_JMillman_AUS_vs_DSchwartzman_ARG_MS159_clip.0020.png\\label{fig:ExampleImage}"}

img <- image_read("Figures/2016_CT6_R01_JMillman_AUS_vs_DSchwartzman_ARG_MS159_clip.0020.png")
   
p <- ggplot() + theme_void() + annotation_custom(rasterGrob(img)) +
      coord_cartesian(xlim=c(0, 800), ylim=c(0,450))
p
```


## Software APIs

Three emotion recognition APIS were considered.
They were chosen for their accessibility and online reviews of their performance, and these are well recognised options for Facial recognition currently available.


```{r Emotion-Software-Specs-Table, echo=F, fig.cap = "\\label{tab:Emotion-Software-Specs-Table}"}

knitr::kable(EmotionSoftwareSpecs, longtable=TRUE, booktabs = TRUE, caption="This details the capabilities we considered important in recognising emotions in images of faces.")
```


As can be seen above, a noticeable difference between the three is the amount of times the API can be called within a given time frame. Skybiometry had the largest imposition on Call Limits as it only allowed 100 API calls to be processed per hour. Microsoft also had a limit imposed, but this allowed for much more to be processed with the possibility of 1200 images to be processed within one hour, after accounting for the wait time between each group of 20.
Google Vision's API call limit had a minimal effect for our purposes.

The number of emotions shown in the results were not congruent. These differences made it necessary to pre process the result to provide the most comparable results for analysis.
Microsoft provided the most emotion options, contempt being the addition that was not provided by Skybiometry. Google provided the least, only giving the options for four different emotions.

The outputs from the APIs were very different. Google provides likelihoods of an emotion occurring on a particular face in categorical options ranging from Very Unlikely to Very Likely. Microsoft provide Proportions, ranging between 0 and , whereas Skybiometry results in a Confidence of the emotion occurring in the specified face in values from 0 to 100.

\newpage
# Procedure


## Process Images

The original image set was subset to create images of faces specifically suited for emotion recognition purposes. 
A crop was performed to create a new image that focused on the area marked as a face by Google^[based on Googleâ€™s bounding box coordinates]. A border to frame the face was also included for visual appeal.
These 1319 new images, of varying sizes, were hosted on Google Drive to allow for URL access from the API to the individual images.


```{r VisualSet, echo=F, fig.cap = "Three Faces in the image set, after extracting faces from the full broadcast video stills, and the dominant emotion each software recognised for them. \\label{fig:VisualSet}"}

plotFaces <- function(imgList){
  for(i in imgList){

  img <- image_read(paste0("Faces/", i))
   
    p <- ggplot(data.frame(x=c(0,350), y=c(0,350)), aes(x,y))+
      theme_void() + 
      annotation_custom(rasterGrob(img)) +
      coord_cartesian(xlim=c(0, 350), ylim=c(0,400))
    
    } 
  return(p)
}

TableEmotions <- function(imgList){
  for(i in imgList){

   d <- DE %>% filter(FileName== i)
  
  dl <- d %>% 
  select(Skybiometry = SEmotion, Microsoft = MEmotion, Google = GEmotion) %>%
    gather(key = Software, value = Emotion, Microsoft, Skybiometry, Google)
   
  t <- tableGrob(dl, theme = ttheme_minimal(core=list(fg_params=list(hjust=0, x=0.1)),
                      rowhead=list(fg_params=list(hjust=0, x=0))), rows = c("", "", ""))
    
  } 
  return(t)
  }

#Faces 
f1 <- "face-1635-1-Go.png"
f2 <- "face-1128-1-Go.png"
f3 <- "face-1746-1-Go.png"

i1 <- plotFaces(f1)
i2 <- plotFaces(f2)
i3 <- plotFaces(f3)

# Tables of their resulting emotion value
t1 <- TableEmotions(f1)
t2 <- TableEmotions(f2)
t3 <- TableEmotions(f3)


#f4 <- "face-626-1-Go.png"
#f5 <- "face-450-1-Go.png"
#f6 <- "face-816-1-Go.png"


grid.arrange(i1, i2, i3, t1, t2, t3, nrow=2)

```


## Process API results

The results extracted from each of the APIs differed, re configuring of these results was needed to create comparable sets for analysis.

Microsoft provided a POST request object in a JSON format. 
This was easily formatted to create a data set of the eight numeric emotion values provided for each face. In the event that emotion values were not returned for a certain face, NAs were presented in the data set.

```{r MEexample, echo=F, message=F, fig.cap = "\\label{tab:MEexample}"}

MEexample <- ME %>% subset(FileName=="face-1745-1-Go.png")

knitr::kable(MEexample[2:9], digits=6, longtable=TRUE, booktabs = TRUE, caption="The Microsoft API provided eight numerical values, one for each emotion.")
```

Skybiometry as provided a POST request object in a JSON format. However, unlike Microsoft, the results included a True or False Value for each of the seven emotions and a confidence level   'confidence value as a percentage from 0 to 100'^[https://skybiometry.com/documentation/#document-21] representing the confidence of the emotion being present on a player's face.

```{r SBexample, echo=F, fig.cap = "\\label{tab:SBexample}"}

SBexample <- SB %>% subset(aname=="face-1745-1-Go.png") %>% select(ends_with(".confidence"))

knitr::kable(SBexample[3:9], digits=6, longtable=TRUE, booktabs = TRUE,
             col.names = c("neutral", "anger", "disgust", "fear",
                           "happiness", "sadness", "surprise"),
             caption="The Skybiometry API provided seven numerical Confidence values, one for each emotion.")
```

The process undertaken to receive results Google produces was the same as the previous APIs. However rather than numeric values, it returned categorical likelihoods for the four emotions it considered.

```{r GEexample, echo=F, fig.cap = "\\label{tab:GEexample}"}
GEexample <- GE %>% subset(FileName=="face-1745-1-Go.png") %>% select(ends_with("Likelihood"))


knitr::kable(GEexample, longtable=TRUE, booktabs = TRUE,
             col.names = c("joy", "sorrow", "anger", "surprise"),
             caption="The Google Vision API provided four likelihood possibilities, one for each emotion.")
```


These differing outputs needed to be arranged in a comparable manner.
Noticing that the sums of the individual emotions for each face in the Microsoft results was approximately one provided the basis for transformation for the other APIs. Dividing the individual Skybiometry emotion confidences by the sum of the confidences would give the proportional likelihood of each emotion being the dominant emotion in a face.

This calculation was simple to perform and resulted in numeric values comparable to Microsoft's values.

Skybiometry Transformed:

```{r SBT, echo=F, fig.cap = "\\label{tab:SBT}"}
SBT <- SB %>% subset(aname=="face-1745-1-Go.png") %>% select(neutral, anger, disgust, fear, happiness, sadness, surprise)


knitr::kable(SBT, digits=6, longtable=TRUE, booktabs = TRUE,
               caption="The Skybiometry API provided 7 numerical Confidence values, one for each emotion.")
```


Google's results then required a more complex transformation for the output to also be comparable. 

As there were five possible categories assigned to each emotion the numeric values were chosen to sit in the middle of five 20 point ranges. This allowed a numerical representation of each possible likelihood value and would allow for numeric comparisons to be made.
The values were assigned according to the table below:

```{r LT, echo=F, fig.cap = "\\label{tab:LT}"}
Likelihood <-c("VERY_UNLIKELY", "UNLIKELY", "POSSIBLE", "LIKELY", "VERY_LIKELY")
Value <- c(10, 30, 50, 70, 90)
LT <- cbind(Likelihood, Value)

knitr::kable(LT, longtable=TRUE, booktabs = TRUE, caption="")
```

As both Microsoft and Skybiometry included a neutral category we considered how to incorporate a neutral category for the Google results.

Our approach considered that when a face was "very unlikely" to be any of the four emotion categories it could be deemed "neutral". However when one emotion was stronger than other it could still be possible that the face only had a small lean toward this emotion and it may not be the obvious choice. Therefore the Confidence value for neutral was calculated according to the formula below: 

$$neutralConf_i = 100 - max(joyConf_i, sorrowConf_i, angerConf_i, surpriseConf_i)$$
  
The process to derive the numeric values for Google followed the process undertaken for Skybiometry values. Where the individual emotion confidences were divided by the sum of the emotion confidence values for each face. This resulted in congruent emotion data.

```{r GET, echo=F, fig.cap = "\\label{tab:GET}"}

GET <- GEP %>% subset(FileName=="face-1745-1-Go.png") %>% 
  select(joy, sorrow, anger, surprise, neutral)

knitr::kable(GET, longtable=TRUE, booktabs = TRUE,
             col.names = c("joy", "sorrow", "anger", "surprise", "neutral"),
             caption="The Google Vision API provided four likelihoods, one for each emotion.")
```



\newpage
# Analysis

Each API returned a different amount of non null results. Microsoft produced emotion results for `r sum(!is.na(ME$anger))` faces. This was more than Skybiometry, as it produced only `r sum(!is.na(SB$anger))` results for emotions on faces. Google provided the most, with  `r sum(!is.na(GE$anger))` emotion results^[As the initial set was faces identified by Google there is the possibility of bias toward being able to identify emotions in faces it was able to find originally.].

 

```{r NonNullFaces, fig.cap = "\\label{fig:NonNullFaces}", results = 'asis'}

J4 <-J4 %>% rowwise() %>%
  mutate(Gna = ifelse(!is.na(Google), TRUE, FALSE)) %>%
  mutate(Sna = ifelse(!is.na(Skybiometry), TRUE, FALSE)) %>%
  mutate(Mna = ifelse(!is.na(Microsoft), TRUE, FALSE)) 

nfft<-ftable(J4$Gna, J4$Mna, J4$Sna, dnn = c("Google", "Microsoft", "Skybiometry"))

library(stargazer)
stargazer(format(nfft, quote=FALSE, justify="right"),title = "Emotion results were found by all three APIs for 364 common faces, none of the APIs produced results for 10 player faces. Google was able to produce 382 results that Skybiometry and Microoft did not.", header=FALSE, type="latex")
```




### Emotion Proportion Distributions
For the following distributions only the faces of players were considered.

```{r GEPhists, fig.cap = "It can be seen that there are significant proportions of faces where the non neutral emotions occur close to zero. This is balanced by the large percentage, over 75%, where the neutral value of just under .75 occurred. The faces used showed higher levels of Joy than any other non neutral emotion as the percentage occuring at zero was less than the percentages of the other emotions.\\label{fig:GEPhists}"}
gganG <- ggplot(GEP, aes(x=anger)) + 
  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")

ggjoG <- ggplot(GEP, aes(x=joy)) + 
  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")

ggneG <- ggplot(GEP, aes(x=neutral)) + 
  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")

ggsoG <- ggplot(GEP, aes(x=sorrow)) + 
  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")

ggsuG <- ggplot(GEP, aes(x=surprise)) + 
  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")

grid.arrange(gganG,ggjoG,ggneG,ggsoG,ggsuG,nrow=2,
             top=textGrob("Google Distribution of Emotion Porportions",
             gp=gpar(fontsize=20,font=3)), 
             left = ("Percentage of Emotion values"))

```



```{r SBPhists, fig.cap = "The distributions show that over 70% of faces had levels of happiness and neutrality at, or close to, 0. The distributions show that there are very little faces with specific emotion levels above 50%. Instead there are low levels of all emotions found on many faces. \\label{fig:SBPhists}"}
gganS <- ggplot(SBP, aes(x=anger))+  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
gghaS <- ggplot(SBP, aes(x=happiness))+  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggneS <- ggplot(SBP, aes(x=neutral))+  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggsaS <- ggplot(SBP, aes(x=sadness))+  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggsuS <- ggplot(SBP, aes(x=surprise))+  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggdiS <- ggplot(SBP, aes(x=disgust))+  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggfeS <- ggplot(SBP, aes(x=fear))+  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) +coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")

grid.arrange(gganS, gghaS, ggneS, ggsaS, ggsuS, ggdiS, ggfeS,nrow=3,
             top=textGrob("Skybiometry Distribution of Emotion Porportions",gp=gpar(fontsize=20,font=3)), 
             left = ("Percentage of Emotion values"))

```



```{r MEPhists, fig.cap="Unlike the previous two softwares, there were many faces that returned high neutral values. This is matched by the high percentages of faces where the proportion levels were at, or close to, zero. While the happiness emotion was most varied, with values between 0 and 1, over 40% of faces returned surprise levels above zero.\\label{fig:MEPhists}"}

gganM <- ggplot(MEP, aes(x=anger)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
gghaM <- ggplot(MEP, aes(x=happiness)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggneM <- ggplot(MEP, aes(x=neutral)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggsaM <- ggplot(MEP, aes (x=sadness)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggsuM <- ggplot(MEP, aes(x=surprise)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggdiM <- ggplot(MEP, aes(x=disgust)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggfeM <- ggplot(MEP, aes(x=fear)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")
ggcoM <- ggplot(MEP, aes(x=contempt)) +  geom_histogram(binwidth=0.055, aes(y=..count../sum(..count..))) + coord_cartesian(xlim = c(0,1), ylim = c(0,1)) + ylab("")

grid.arrange(gganM, gghaM, ggneM, ggsaM, ggsuM, ggdiM, ggfeM, ggcoM,nrow=3, 
             top=textGrob("Microsoft Distribution of Emotion Porportions", gp=gpar(fontsize=20,font=3)),
             left =("Percentage of Emotion values"))


```

\newpage

###Predominant Emotions
To contrast the emotion results for certain faces a 'predominant' emotion was designated to each face. Where the emotion with the maximum proportion value was deemed the 'predominant' emotion for a particular face.

This resulted in one of seven emotions for Google, one of eight for Microsoft, and one of five for Google. This also required the assumption that joy is equivalent to happiness, and sorrow to sadness.

```{r predominant, results = 'asis', fig.cap = "\\label{tab:predominant}"}
#Table including Google
#ftable(DE$GEmotion, DE$SEmotion, DE$MEmotion, dnn = c("Google", "Skybiometry", "Microsoft"))

ft<-ftable( DE$MEmotion, DE$SEmotion, dnn = c("Microsoft","Skybiometry"))

stargazer(format(ft, quote=FALSE, justify="right"),title = "This shows the agreement between the emotions recognised by Microsoft and Skybiometry", header=FALSE, type = "latex")
```

There was only one occurence of all three APIs gaving the same emotional result. This occured when there was agreement for 11 faces that the dominant emotion was anger. 
In Table \@ref(tab:predominant) where Skybiometry emotion recognitions are shown in the rows, and Microsoft's in the columns. 
It is reasonable to only consider Microsoft and Skybiomertry it can be seen that they agreed 408 faces were predominantly showing anger, and 12 were showing surprise.


As neutral was a prominent category for Microsoft and Google, we considered where neutral was the dominant emotion and grouped the remaining emotions into a non-neutral category.


```{r neutral-non, results='asis', fig.cap="\\label{tab:neutral-non}"}
DEne<-DE %>% rowwise()%>%
  mutate(GEneutral = ifelse(GEmotion=="neutral", "neutral","non-neutral")) %>%
  mutate(SEneutral = ifelse(SEmotion=="neutral", "neutral","non-neutral")) %>%
  mutate(MEneutral = ifelse(MEmotion=="neutral", "neutral","non-neutral")) 

# neutral flat table
nft<-ftable(DEne$GEneutral, DEne$MEneutral,DEne$SEneutral, dnn = c("Google","Microsoft","Skybiometry"))

stargazer(format(nft, quote=FALSE, justify="right"), header=FALSE, type = "latex", title ="This shows the agreement between non-neutral and neutral faces recognised by Goole, Microsoft and, Skybiometry. Only three faces were categorised as neutral. Of these, Google was much more likely to categorise as neutral.")
```

The following Figure \@ref(fig:neutrality) displays a Scatter Plot Matrix that captures the pairwise variablitiy in the Neutral proportion levels across APIs.


```{r neutrality, fig.cap = "The Scatter Plot Matrix shows the variablitiy in the Neutral proportion levels. The -0.01 correlation value for Google and Skybiometry show that they do not behave either in a similar way, or inversely. They are uncorrelated.\n Some high values of neutrality for Google are matched to some high levels found by Skybiometry. However, in the lower ranges of both there is a lot of variability in proportion values.\\label{fig:neutrality}"}

library(GGally)
ggscatmat(as.data.frame(na.omit(J4[,c(2:4)])), alpha = 0.4)
```




\newpage
## Validation
We considered a subset of images where the emotions on the faces were manually considered.


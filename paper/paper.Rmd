---
title: "Performance Comparison of Common Face Detection APIs using Video Stills from Professional Tennis Matches"

# to produce blinded version set to 1
blinded: 1

authors: 
- name: Stephanie Kobakian
  thanks: The authors gratefully acknowledge Tennis Australia providing data for this project.
  affiliation: Monash University 
  
- name: Mitchell O'Hara-Wild
  affiliation: Monash University 

- name: Dianne Cook
  affiliation: Monash University 

- name: Stephanie Kovalchik
  affiliation: Victoria University and Tennis Australia 

keywords:
- data science
- sports analytics
- shiny
- R
- statistics
- glm
- data visualization

abstract: |
  This paper examines the effectiveness of facial detection APIs on broadcast video stills of Australian Open tennis matches. The goal is to determine the best API to use for face detection of players throughout a match. For training purposes, faces were manually tagged in 6406 images, using a specially constructed web application, recording the scene characteristics and features of individual faces, such as head wear or sunglasses. API performance was evaluated on their success rate at detecting the tagged faces, and also relative face and scene attributes. 

toc: false
fig_caption: yes
number_sections: false
fontsize: 11pt
bibliography: references.bib
biblio-style: chicago
output: rticles::asa_article
link-citations: true
---


```{r cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
#packages:
library(bookdown)
library(pander)
library(ggplot2)
library(knitcitations)
library(RefManageR)
library(readr)
library(knitr)
library(grid)
library(kfigr)
library(UpSetR)
library(descr)
library(xtable)
library(gtable)
library(ggthemes)
library(EBImage)
library(gridExtra)
library(ggmosaic)
library(tidyverse)
library(purrr)
library(rticles)

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, kfigr.link=TRUE, kfigr.prefix=TRUE, cache=TRUE, fig.env = TRUE, fig.cap=TRUE)

 
options("citation_format" = "pandoc",knitr.table.format = "latex")

BibOptions(check.entries = FALSE, style = "markdown", bib.style = "alphabetic", cite.style = 'alphabetic', longnamesfirst=T)


#csv files:
amin <-read_csv("data/ALLmetaIMGnamed.csv", col_types = cols(type = col_factor(levels = c("Manual", "Animetrics", "Google", "Microsoft", "Skybiometry")))) %>% 
  filter(!duplicates)
SceneAttributes<-read_csv("data/SceneAttributes.csv")
FaceAttributes<-read_csv("data/FaceAttributes.csv")


aminFaces <- amin %>%
  filter(!is.na(type)) %>% 
  mutate(fileID = as.numeric(factor(file))) %>%
  mutate(FaceKey=paste(fileID, boxID, sep="-")) %>%
  mutate(FaceID=paste(fileID, boxID, substring(type, 1,2), sep="-"))


# colour scheme
type.colours <- c(Animetrics =     "#7BCCC4",
                      Google =     "#FC9272",
                      Manual =     "#C994C7",
                      Microsoft =  "#FA9FB5",
                      Skybiometry ="#ADDD8E")
   
#plot images with overlaid boxes
overlayGgplot <- function(imgList, mergeData, matchBox=FALSE, colourScheme = type.colours, legend=TRUE){
  for(i in imgList){
    
    #read in image
    image <- readImage(paste0("figures/", i))
    #used in analyses
    #image <- readImage(paste0("images/", i))
    
    #convert image to a df, add hex value
    image_df <- data.frame(x=rep(1:nrow(image), ncol(image)),
                           y=rep(1:ncol(image),
                                 rep(nrow(image),
                                     ncol(image))),
                           r=as.vector(image[,,1]),
                           g=as.vector(image[,,2]),
                           b=as.vector(image[,,3]))
    image_df$h <- rgb(image_df[,3:5])
    
    #Create the plot of the image
    p <- ggplot() +
      scale_fill_identity() + 
      geom_tile(data=image_df, aes(x,-y, fill=h)) + 
      theme_void()
    
    # Find associated Face Box information for specific image
    faceData <- aminFaces %>% filter(file == i)
    
    if(nrow(faceData) == 0){
      return(p)
    } else{
      
      faceData <- faceData %>%
        mutate(x1 = minX, x2 = minX, x3 = maxX, x4 = maxX, x5 = minX,
               y1 = minY, y2 = maxY, y3 = maxY, y4 = minY, y5 = minY) %>%
        gather(corner, boxPos, x1:y5) %>%
        mutate(posCoord = substr(corner, 1, 1), posID = substr(corner, 2, 2)) %>%
        dplyr::select(-corner) %>% spread(posCoord, boxPos) %>% 
        dplyr::select(file, type, minX, maxX, minY, maxY, FaceKey, FaceID, posID, x, y)
    }
    
  }
  if(matchBox){
    p1 <- p + geom_rect(data=faceData, 
                        aes(xmin=minX, ymin=-maxY, xmax=maxX, ymax=-minY, 
                            color=FaceKey, group=FaceID), fill=NA) +
      guides(colour = "none")
  }
  else {
    pa <- p + geom_rect(data=faceData, 
                        aes(xmin=minX, ymin=-maxY, xmax=maxX, ymax=-minY, 
                            color=type, group=FaceID), fill=NA)  +
      coord_fixed(ratio=0.85) + scale_color_manual(values = colourScheme)
    
    if (legend) p1 <- pa + guides(colour = guide_legend("")) + theme(legend.position = "bottom")
    else p1<- pa + theme(legend.position = "none")
    }
  
  return(p1)
}


# Create graphs of factors in manual proportions
getManualCount <- function(type, count) {
  return(count[type == "Manual"])
}

ggplotProportion <- function(dataset, factorVar){
  factorVar <- deparse(substitute(factorVar))
  dataset <- dataset %>% filter(matchesManual) %>% group_by_(factorVar, "type") %>% summarise(nTotal=n()) %>% group_by_(factorVar) %>% mutate(ManualCount = getManualCount(type, nTotal)) %>%
    mutate(proportion = nTotal/ManualCount) %>% rename_(xvar = factorVar) %>% filter(type!="Manual")
   ggplot(dataset, aes(x=xvar, y=proportion, group = reorder(type, -proportion), fill=type)) + geom_bar(stat = "identity", position = "dodge") +  ylab("Proportion of faces matched") + xlab(factorVar) + scale_colour_manual(values=type.colours)
}


ALLmetaIMGPlayers <- amin %>% filter(detect=="Player")


# Create UpSetR graph
createUpSet <- function(data) {
VennLists<-data %>%
  split(.$type) %>% 
  map(~ .$FaceKey)%>% 
  fromList() %>%
  upset(order.by="freq", nsets=100)
}


```



#Introduction

The presence of emotions during professional tennis matches and the influence they have on athletes is often discussed in commentary, by players and coaches alike.
However there is little quantitative evidence to support the theories and suggestions that the presence of emotions improves or impedes the performance of players. 
As many tennis professionals believe it is a game heavily influenced by the mental states of the players there is an opportunity to analyse this "inner game", with the hope of improving the performance, and coaching of, tennis players.
By statistically analyzing the faces and expressions of players during a match, insights may be gained about the mental state of a player and the effects changes in the mental state have on the outcome of a match. The aim of this study is to develop methods to collect accurate information about the facial expressions of elite tennis athletes during match play.


Application Programming Interfaces (APIs) have been used to evaluated the performance of several popular facial recognition software, on the still images derived from broadcast videos of elite tennis matches. 
While it is difficult to know the thoughts and feelings of a player during a match, analysts may be able to gain information through results produced by recognition software. This approach to understanding player's emotions during a match differs to previous standards that have used player's recollections after a game to understand their emotions.

Most facial recognition software began with the intention of security applications. There are others, such as Facebook which links images of people with their profiles.
This means the recognition software currently available was not intended to be used in fast paced, elite sport environment.
It was believed that the capabilities of the software could be limited by their intended security and surveillance purposes. @Boston addresses the 'lack of robustness of current tools in unstructured environments' and this survey explores how detection performance is affected by situations that arise in elite tennis matches.


The aims of the present study were to determine the feasibility of using currently available APIs for extracting facial information of players during broadcasts of professional matches, by comparing the performance of several popular facial recognition APIs. A limited selection of accessible APIs was chosen based on their ability to produce appropriate and useful facial recognition. The performance was evaluated against manual classifications obtained in an annotation tool developed by the authors. 

# Methodology

In this study any reference to a 'face' is considered to be an area designated manually or by an API as an area that encloses a human face. These may or may not be actual faces.

##Sample and sampling approach

Images from the Australian Open 2016 were provided by Tennis Australia, with goal being that the sample is representative of the video files to be used for future facial recognition analysis: 6406 images, 800 $\times$ 450px 

To collect the images, 5 minute segments were taken from 105 video files, with a still shot stored every three seconds. The video files used were the broadcasts of the tennis matches shown on the Seven Network during the Australian Open 2016. The sample included an equal amount of females and males singles tennis matches. The rounds of the competition vary so as to not limit the pool of players to only those who progressed, though there was a higher chance of advancing players reappearing. 

The sample included images that contained the faces of many people, such as players, staff on the court and fans in the crowd. All of these faces were included in the manual annotations as they were likely to be found by the software selected. It was decided that including these additional faces would allow better evaluations of the software's capabilities, and provide information to differentiate between players and other faces captured. Therefore the sample was not filtered at this initial stage.

Matches played during the Australian Open are played on a range of courts available at Melbourne Olympic Park. The sample was selected to be representative of the seven courts that have the Hawk Eye technology enabled.


##Software selection

The initial software to be considered was informed by a report that reviewed 'commercial off-the-shelf (COTS) solutions and related patents for face recognition in video surveillance applications' [@Survey, pp.3].
The selection criteria included the availability, speed, feature selection and whether images and/or videos could be presented for detection. The results of Gorodnichy, Granger and Radtke's [-@Survey] report considered processing speed, feature selection techniques, and the ability to perform both still-to-video and video-to-video recognition.

The report outlined that Animetrics [@Animetrics] required an 'image/face proportion should be at least 1:8 and that at least 64 pixels between eyes are required'. 
SkyBiometry [@Skybiometry] is a 'spin-off of Neurotechnology' which was considered by Gorodnichy et al. [-@Survey].
Companies who have recently expanded their API ranges were also considered. This search produced two of the software, Microsoft API [@Microsoft], provided by Microsoft Cognitive Services, and Google Vision API [@Google].
Online demos were used to test viability, Figure \ref{fig:Trial-Image} depicts Bernard Tomic and the detected areas found.


```{r Trial-Image, fig.cap = "This image of Bernard Tomic was chosen as a trial image to be presented to each of the software before they were included in the research. Each colour represents a different detection source. It was expected that the software would be able to find this face, despite the player facing away from the camera.\\label{fig:Trial-Image}", message=TRUE, warning=TRUE, dev="jpeg", dpi=300}
imagesList0<-as.list("2016_HSA_R01_BTomic_AUS_vs_DIstomin_UZB_MS157_clip.0013.png")
overlayGgplot(imagesList0, aminFaces, matchBox = FALSE)
```



## Manual annotations 


```{r Scene-Attributes-Table, fig.cap ="\\label{tab:Scene-Attributes-Table}"}
kable(SceneAttributes, format = "latex", longtable=FALSE, booktabs =TRUE, caption = "Characteristics of the scene occuring when the still shot was taken, one option for each category was selected for each image.")
```


A web based annotation tool was developed to allow image annotations that would capture the location of areas selected manually, and annotation of attributes for each face, and scene. Specific information for each face within each scene was collected. To determine which of the, sometimes many, faces in the scene it would be reasonable for software to detect, and to minimize human labor, some rules were applied to the manual tagging: 

\begin{itemize}
\item The faces were recorded if it showed their face at a minimum of 20 by 20 pixels. 
\item The back of the head was not detected as a face by any software, these areas were classified manually but reclassified as other.
\item Crowd faces were not the intended targets of the recognition however these faces contributed to our understanding of the software. 
\item The same face size standard applied to crowd members, but focus was placed on the most prominent faces. 
\end{itemize}

Table \ref{tab:Face-Attribute-Tables} summarises attribute information recorded for each face.

```{r Face-Attribute-Tables, results='asis', fig.cap = "\\label{tab:Face-Attribute-Tables}"}
kable(FaceAttributes, format = "latex", longtable=FALSE, booktabs = TRUE, caption = "Characteristics of the faces captured, one option for each category was selected for each face manually tagged.")
```

The web app was created using the Shiny package for R [@shiny] and has been developed into an R package called taipan, as it is a Tool for Annotating Images in Preparation for ANalysis. The annotator was able to highlight the section of the image containing a face. This selection activated a set of attributes questions for the annotator to answer, and allowed information to be recorded for the face. The records included the location of the box in the image, and answers to the questions providing the attributes of the face. 
When a face was not visible in the image only the scene attributes were recorded for the image. All of the annotations for this sample were completed by one annotator to provide a consistent sample of faces annotated manually. The initial decisions of what would be reasonably detected was made by several people.


##Software usage

The face recognition software APIs were accessed via a R [@R] script, calling the APIs relied on the httr [@httr] package. The scripts looped through the images, individually posting a request to retrieve information provided and convert it into an appropriate format for analysis. Special handling was required due to the limits on requests per minute of Skybiometry and Microsoft, time-controlling and error management of the requests was incorporated for these APIs.

Figure \ref{fig:timeFigure} shows the distribution of processing time required by the APIs for each image. The violin plot display created by @Violin 'combines the box plot and the density'. A categorical variable splits the data along the x axis, as would be seen in a box plot display. A density plot is then incorporated on the y axis. This is produced by manipulating a density display; switching the axes allows the continuous variable to increase along on the y axis, then the violin shape is created by mirroring the density. The width of the violin will now be twice the length that would be have been measured vertically in a density plot. 


As seen in the facets of Figure \ref{fig:timeFigure}, both Microsoft and Google had few images which took a significant amount of time to complete, and these were removed from this plot due to concerns regarding the network. The times taken to search are quite variable between APIs, with Skybiometry consistently spending less time than other APIs. Microsoft and Google's times vary regardless of the number of faces found. The lines overlaid in each plot represent the smoothed linear model of the amount of faces, it can be seen that all APIs take longer to complete detection of faces in an image when there are more faces. As all APIs had more images where only one or two faces were found it resulted in more variability in the amount of time taken.


```{r timeData}
times <- amin %>% 
  filter(type!="Manual") %>% 
  select(file, type, time.user.self, time.sys.self) %>% 
  group_by(file, type) %>% 
  mutate(n=n()) %>% 
  distinct() %>% ungroup()
```

```{r timeFigure, fig.cap="System time taken by the four APIs to finish searching individual images, by number of faces discovered, displayed as violins. The width of the violin shows the most common times for the group of images.\\label{fig:timeFigure}"}

# violin plot
t2 <- ggplot(times, aes(x= as.factor(n), time.sys.self, fill=type)) +
  geom_violin(scale = "width", alpha=0.5) + ylim(0,0.08) +
  #geom_jitter(height = 0.005, width = 0.1, alpha=0.5) +
  facet_wrap(~ type) +
  xlab("Amount of faces found") +
  ylab("Time taken per image (Seconds)") + scale_fill_manual("", values=type.colours) +
  geom_smooth(aes(x=n, time.sys.self), method="lm", se=F, colour="black") + 
  scale_colour_manual("", values=type.colours)

t2
``` 


```{r eval=FALSE}
times %>% 
  split(.$type) %>%  map_df(~ summary(.$time.user.self, digits=max(2, getOption("digits")-2))) %>%
  rownames_to_column %>%  gather(var, value, -rowname) %>% 
  spread(rowname, value) %>% select(c("API" = `var`,
                            "Min" = `1`,
                        "1st Qrt" = `2`,
                        "Median"  = `3`,
                        "3rd Qrt" = `5`,
                        "Max"     = `6`)) -> usertime
times %>% 
  split(.$type) %>%  map_df(~ summary(.$time.sys.self, digits=max(2, getOption("digits")-2))) %>%
  rownames_to_column %>%  gather(var, value, -rowname) %>% 
  spread(rowname, value) %>% select(c(
                            "Min" = `1`,
                        "1st Qrt" = `2`,
                        "Median"  = `3`,
                        "3rd Qrt" = `5`,
                        "Max"     = `6`)) -> systime

cbind(usertime, systime) -> timetable

kable(timetable, booktabs = TRUE, longtable=TRUE, caption="This table allows comparisons to be made between the  amount of time taken for each API took to detect faces in each image. Most of the images take less than one second to process. Google had the highest maximum time of 4.936.", format = "latex") #%>%
 # add_header_above(c(" " = 1, "Group 1" = 5, "Group 2" = 5))
```

It should be considered that the time taken may be an issue for 'real time' processing. It would depend on the amount of frames sampled. This also does not account for the time taken to process the information returned.


##Data processing

The data needed for our analysis was organised by source, separate files for each of the four APIs. They contained the information on the location of the faces found in the images, and the time taken to find them. Some APIs also provided detailed information such as the estimated head angle, and locations of specific facial features. The manual tagging app resulted in two files, one containing information about the scene, and the other contained information regarding the faces.

The location and size of the boxes around the faces were recorded, these values were used to see if a particular identified face box matched a manually identified face, or a region found by another software. Every face found in an image manually, or by any API was compared. From their locations the intersecting area between them was found. When the ratio of this intersecting area to total area was greater than 0.1 the IDs were altered to reflect the two faces would be considered the same face. This allowed comparison of the identification areas on the same face, as well as contrast the identified faces of each software.

##Results

The manual tagging helped to provide a benchmark for faces found by APIs. The
face detection rate indicated what attributes of an image or face were associated with face detection by an API. 
Faces that were found by multiple APIs indicated either obvious facial features were visible or the APIs may be using similar criteria to detect faces. This application allowed for a broader range of angles, backgrounds and accessories that may have impacted detection.
False discoveries were the instances of faces found by APIs that did not intersect with manually tagged faces. Some of these may actually have been faces or reveal sensitivities in the algorithms of the APIs.
The analysis of image characteristics and accessories indicates whether the APIs were viable choices for detecting faces in sports environments. 




### Modeling detection rate

A logistic regression model was used to assess the characteristics of the face and scene that may have affected the detection likelihood of the face. A step-wise variable selection method was used to find variables of significance for any of the APIs. The binary response was whether the face was detected or not, using the manually tagged data as the benchmark, and characteristics included the setting, angle of shot and player, background, lighting and player adornments. Variables that were not included denoted whether there was a graphic or live shot visible, the role of the person whose face was captured and whether or not there was a person in the image.
detect
A separate model was fit using the faces for each API.

```{r RegressionFunctions, cache=FALSE}
# Prepare data
hitmiss <- function(x){
  allType <- c("Animetrics", "Google", "Microsoft", "Skybiometry")
  hit <- allType %in% x$type  
  x[1,] %>%
    dplyr::select(file:visorhat) %>%
    cbind(type = allType, hit = hit)
}


GlmModelCreation <- function(model, data = aminFaces) {
  glmFits <- data %>% 
  split(.$FaceKey) %>% 
  map_df(~ hitmiss(.)) %>% 
  split(.$type) %>% 
  map(~ glm(model, data = dplyr::select(., -type, -file), binomial(link = "logit")))
}


ConvertModel2Table <- function(model){
  model %>%
    summary %>%
    coef %>%
    as.data.frame %>%
    cbind %>%
    rownames_to_column %>%
    cbind %>%
    dplyr::rename(variable = rowname)
}


GlmModelEstimates <- function(model, data = GlmModelCreation(model)){
  glmSummary <- data %>% 
    map(~ ConvertModel2Table(.)) 
  
  glmPlot <- do.call(rbind, Map(cbind, glmSummary, type = names(glmSummary))) 
  return(glmPlot)
}


# SignificancePlot <- function(model, data = GlmModelEstimates(model)) {
#   data %>%
#   mutate(significant = `Pr(>|z|)` < 0.05) %>%
#   ggplot(aes(x=type, y=`Pr(>|z|)`)) +
#   geom_col(aes(fill=significant)) + 
#   facet_wrap(~ variable) +
#   coord_flip()
# }

EstimatesPlot <- function(model, data = GlmModelEstimates(model)) {
  data %>%
  mutate(significant = `Pr(>|z|)` < 0.05) %>%
  ggplot(aes(x=type, y=Estimate)) +
  geom_col(aes(fill=significant)) +
  facet_wrap(~ variable, scales="free_x", labeller=label_wrap_gen(width = 25, multi_line = TRUE)) +
  coord_flip()
}


ModelPlotResults<-function(model, data = GlmModelEstimates(model)){
  ep<-EstimatesPlot(model, data)
  #sp<- SignificancePlot(model, data) 
  grid.arrange(ep)#, sp)
}
```



```{r ModelEstimates, fig.cap ="\\label{tab:ModelEstimates}"}
TypeModel <- hit ~ 0 + 
              shotangle + 
              bg + 
              bg*shotangle + 
              situation + 
              obscured +
              headangle +
              lighting + 
              glasses + 
              visorhat

TypeGlms <- GlmModelCreation(model=TypeModel)
ModelEstimates <- GlmModelEstimates(data=TypeGlms)

#Create estimates columns
ModelEstimates %>% mutate(Estimate = round(Estimate,2)) %>%
select(variable, type, Estimate) %>%
  spread(key=type, value=Estimate) %>%
  select(variable, A.e=Animetrics, G.e=Google, M.e=Microsoft, S.e=Skybiometry)-> ModelEstimatesTypes 

#Create significance columns
ModelEstimates %>% mutate(sig = ifelse(`Pr(>|z|)` <= 0, "***", 
                            ifelse(`Pr(>|z|)` <= 0.001, "** ",
                            ifelse(`Pr(>|z|)` <= 0.010, "*  ",
                            ifelse(`Pr(>|z|)` <= 0.050, ".  ", 
                                                        "   ")))))  %>%
select(variable, type, sig) %>% 
  spread(key=type, value=sig) %>% 
  select(variable, A.s=Animetrics, G.s=Google, M.s=Microsoft, S.s=Skybiometry) -> ModelEstimatesSign 


ModelEstimatesTable <- as.data.frame(cbind((ModelEstimatesTypes), ModelEstimatesSign[,2:5])) %>%
mutate(A = ifelse(A.s=="   ", "-", format(round(A.e, 2), nsmall = 2)),
       G = ifelse(G.s=="   ", "-", format(round(G.e, 2), nsmall = 2)),
       M = ifelse(M.s=="   ", "-", format(round(M.e, 2), nsmall = 2)),
       S = ifelse(S.s=="   ", "-", format(round(S.e, 2), nsmall = 2))) %>% 
  select(variable, A, ` ` = A.s,
         G, `  ` = G.s,
         M, `   ` = M.s,
         S, `    ` = S.s)

  
kable(ModelEstimatesTable, format = "latex", align=c('l', 
                                           'r', 'l',
                                           'r', 'l',
                                           'r', 'l',
                                           'r', 'l'),
             booktabs = TRUE, longtable=FALSE, caption="Evaluation of factors affecting face recognition for API, as assessed by logistic regression model. Coefficients from the four models shown (significance: ** 0.01 * 0.05 . 0.01 - NS). Most APIs wre affected by similar factors, with glasses, visor or hat, head angle, lighting, shot angle and situation all contributing significantly to face detection.")
```


For the most part, the APIs were all affected by the same factors. Those that negatively affected all APIs were glasses, visor or hat. Background only negatively affected Google's detection rate. Situation had different effects on each API.
Results for Google were most interesting in the shot angle had differing effects on the detection. 


```{r UnlikelyFace, fig.cap = "The face captured manually in this image was not captured by any API. The head angle, shot angle, lighting, wearing the visor and being captured across the court during the match all contribute to the struggle of the APIs.\\label{fig:UnlikelyFace}", dev="jpeg", dpi=300}

predSet <-
aminFaces %>% select(file,glasses, headangle, lighting, obscured, shotangle, situation, visorhat, bg) %>%
  filter(headangle=="Profile" &
         situation=="Match play" &
         lighting=="Shaded" &
         shotangle=="Upward" &
         bg=="Crowd" &
         obscured=="Yes" &
         visorhat=="Yes")

overlayGgplot(predSet$file)
```


```{r Regression, fig.width=7.5, fig.cap = "The figure depicts the change in the probability of a face being detected by the APIs given a certain attribute level associated with the face. Wearing either glasses, or headwear, a visor or a hat, significantly decreased the likelihood of a manually annotated face being detected by the APIs.\\label{fig:Regression}", eval=FALSE}

#unnecessary as table is included
#ModelPlotResults(hit ~ shotangle + bg + bg*shotangle + situation + lighting + glasses + visorhat)

```



```{r GoogleRegression, fig.cap = "Google Regression\\label{fig:GoogleRegression}"}
aminFaces %>% filter(type=="Manual" | type=="Google") %>%
  mutate(isplayer = ifelse(detect=="Player", 1, 0)) -> players

mod1 <- glm(isplayer ~ 0 + 
              shotangle + 
              bg + 
              bg*shotangle + 
              situation + 
              obscured +
              headangle +
              lighting + 
              glasses + 
              visorhat, data=players, family=binomial(link="logit")) 

#drop1(mod1, test="Chisq")
#summary(mod1)
# currently same model as earlier, give two?

```


When forcing no intercept, all variables are significant at the 0 level, and obscured is only significant at the 0.001 level. 



### Detection rates and API comparison

Each API returned areas in the image that indicated the face location. These are defined using the four points, marking the four corners of a rectangle.  

```{r FaceData, eval=FALSE, fig.cap = "\\label{fig:FaceData}  The bar chart of the number of faces detected by each API. Google's Facial Recognition API recognized almost 1000 more faces than the next best API, Microsoft."}

ggplot(amin, aes(x = type, fill=type)) + 
  geom_bar(position="dodge") +
  xlab("Facial Recognition APIs") +
  scale_x_discrete(limits=c("Manual","Google","Microsoft","Skybiometry","Animetrics")) +
  ylab("Number of Faces") + 
  guides(fill=FALSE) +
  labs(caption="") + scale_fill_manual(values=type.colours)
```

Table \ref{tab:facecounts} displays the proportion of the manually tagged faces that the APIs detected. Google had by far the best detection rate. In addition, the APIs also detected non-faces, in images that produced no manually tagged faces. Google had the highest of these, and the detections were for the most part helpful, primarily of tiny faces in the crowd, that were ignored by manual tagging. They would not be needed for the long term purpose of the study, to detect and tag players' emotions during a match.

In comparison, examining the `r amin %>% filter(type=="Animetrics",!matchesManual) %>% nrow()` images in which Animetrics detected faces, not reported manually, revealed surprising results, which are discussed in more detail later.


```{r table}
FacesAPI <- aminFaces %>% 
  filter(!duplicates) %>%
  select(FaceKey, type) %>%
  table() %>%
  as.data.frame() %>%
  spread(key=type, value=Freq) %>%
  mutate(Animetrics = ifelse(Animetrics==0, "No Face", "Face"),
         Google = ifelse(Google==0, "No Face", "Face"),
         Microsoft = ifelse(Microsoft==0, "No Face", "Face"),
         Manual = ifelse(Manual==0, "No Face", "Face"),
         Skybiometry = ifelse(Skybiometry==0, "No Face", "Face")) 

GM <- CrossTable(FacesAPI$Manual, FacesAPI$Google, prop.c = F, prop.t = F, prop.chisq = F)
MM <- CrossTable(FacesAPI$Manual, FacesAPI$Microsoft, prop.c = F, prop.t = F, prop.chisq = F)
SM <- CrossTable(FacesAPI$Manual, FacesAPI$Skybiometry, prop.c = F, prop.t = F, prop.chisq = F)
AM <- CrossTable(FacesAPI$Manual, FacesAPI$Animetrics, prop.c = F, prop.t = F, prop.chisq = F)
```

\begin{table}[htp]
%\parbox{.5\linewidth}{
\caption{Capture rates of the APIs relative to manually tagged faces. Google tagged twice as many of the manual faces found than the next best API. It also had a larger proportion of Non-faces found, all of these faces were actual faces, usually of crowd members. However many of the faces found by other APIs were not actually faces, instead areas contained the net, clothing and logos.}
\bigskip
\label{tab:facecounts}
\centering
\begin{tabular}{|l|rr|rr|}  \hline
& \multicolumn{2}{r|}{Faces found}&\multicolumn{2}{r|}{Non-faces found}\\\cline{2-5}
API & Count & Prop & Count & Prop \\\hline
Google & 1319 & 0.57 & 638 & 0.49 \\
Microsoft & 565 & 0.25 & 505 & 0.38 \\
Skybiometry & 493 & 0.21 & 512 & 0.39 \\
Animetrics & 434 & 0.19 & 527 & 0.40 \\\hline
\end{tabular}
\end{table}
  
```{r FaceUpSet, fig.cap = "\\label{fig:FaceUpset} Set visualization, to examine intersections between API and manual face detections:  bar charts show counts, by API (lower left) and for combinations (main part of plot), and dot display (bottom) indicates the group intersections. All five way intersections are examined, but only the first few have large counts. The largest group is the manual tagging with 946 faces uniquely identified. The intersection between manual tagging and Google is 716 faces. The intersection of all four APIs and manual tagging is fourth highest, at 342."}
setdata <- aminFaces %>% 
  mutate(count = 1) %>%
  select(type, FaceID, count) %>%
  separate(FaceID, c("id1", "id2", "API")) %>%
  unite(id, c("id1", "id2")) %>%
  spread(API, count, fill=0) %>% 
  arrange(id) %>% 
  group_by(id) %>%
  summarise(Animetrics=as.integer(sum(An)), 
            Google=as.integer(sum(Go)), 
            Manual=as.integer(sum(Ma)),
            Microsoft=as.integer(sum(Mi)), 
            Skybiometry=as.integer(sum(Sk))) 
  
setdata <- setdata %>% mutate(id=factor(id)) 
setdata <- as.data.frame(setdata)

upset(setdata, order.by="freq")
# colsum <- apply(setdata[,-1], 2, sum)
# rowsum <- apply(setdata[,-1], 1, sum)
# onlyone <- setdata[rowsum==1,] 
# apply(onlyone[,-1], 2, sum)
```


Figure \ref{fig:FaceUpset} is a set visualization to examine the intersection of the APIs, and the manual annotations. This is produced with the UpSetR [@UpSetR] package. The black bubbles below the bars indicate the API combination that corresponds to the bar count. The largest count comes from manual tagging, followed by Google and manual tagging, and then Google. Faces detected by all four API and manual annotations were the fourth largest count. This says that the APIs agreed on about a quarter of the faces from the manually tagged collection. The fifth group was the most surprising -- these were faces captured by Animetrics alone, that were false discoveries.

## False discoveries

Animetrics provided many surprises. Figure \ref{fig:UnusualImages} shows four images and the captures where non-faces were detected as faces. In image (A) the player's back is detected as a face.
In image (B) the KIA logo was reported as a face. 
In image (C), where logos on the player's and ball boy's shirts were returned as faces. In image (D) where there are a lot of crowd faces, if you look closely one of the detections is actually a fist (to the right on the center).

The other APIs also returned some non-faces, but the most egregious mistakes came from Animetrics.

```{r UnusualImages, fig.cap = "Four images in which Animetrics located unusual faces. \\label{fig:UnusualImages}", dev="jpeg", dpi=300}
i1 <- "2016_CT6_R01_CGarcia_FRA_vs_BStrycova_CZE_WS145_clip.0015.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(A)")
i2 <- "2016_HSA_R03_FDelbonis_ARG_vs_GSimon_FRA_MS302_clip.0072.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(B)")
i3 <- "2016_CT6_R02_ABeck_GER_vs_TBacsinszky_SUI_WS220_clip.0017.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(C)")
i4 <- "2016_SC2_R01_ATomljanovic_AUS_vs_KBondarenko_UKR_WS1112_clip.0053.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(D)")

gridExtra::grid.arrange(i1, i2, i3, i4, nrow=2)
```



```{r Image-Characteristics-Table, fig.cap ="\\label{tab:Image-Characteristics-Table}"}
ImageCharacteristics <- amin %>% filter(type=="Manual") %>% group_by(situation, bg, shotangle, detect) %>% dplyr::summarise(count=n())
ImageCharacteristics<-arrange(ImageCharacteristics, desc(count))
ImageCharacteristics<-ImageCharacteristics[1:5,]
kable(ImageCharacteristics, booktabs = TRUE, longtable=FALSE, format.args=list(width=8), caption="Top five most frequent image attribute combinations.", format = "latex")
```


```{r ICFigs, fig.height=7, fig.cap = "Relationships between court and face characteristics, and API detection, using mosaic plots: (A) Angle of face capture, (B) court situation, (C) angle of capture by background, and (D) situtation of capture by background. Most face captures occurred at a level angle, and this produces slightly better detection. Most situtation face captures are close up, but the detection rate is identical regardless of situation. Most level angles had the logo wall in the background, while most birds-eye had the court as background, as we would expect. Close ups were mostly against the logo wall. \\label{fig:ICFigs}"}

aminFaces$shotangle <- factor(aminFaces$shotangle, 
                              levels = c("Birds Eye","Upward","Level"))
aminFaces$matchesManual <- factor(aminFaces$matchesManual, levels=c(TRUE, FALSE))

MosSaMm <- ggplot(data=aminFaces) + 
  geom_mosaic(aes(x=product(shotangle), fill=matchesManual)) +
  #geom_text(aes(label="test", x=1, y=0.6)) +
  scale_fill_brewer(palette="Dark2") +
  guides(fill = guide_legend(title="API matches manual"))  +
  labs(caption="") + 
  scale_x_productlist(labels=c("Birds Eye"="Birds \nEye",
                            "Upward"="Upward \nAngle",
                            "Level" =
                            "Level")) + coord_flip() +
  ylab("Proportion of faces") + xlab("Angle of face capture") + 
  theme(legend.position = "none") + 
  ggtitle("(A)") 


aminFaces$situation <- factor(aminFaces$situation, levels = c("Not player",
          "Match play",
          "Off court",
          "Crowd",
          "Close-up"))
aminFaces$matchesManual <- factor(aminFaces$matchesManual, levels=c(TRUE, FALSE))
  
MosSitMm <- ggplot(data=aminFaces) + 
  geom_mosaic(aes(x=product(situation), fill=matchesManual)) +
  scale_fill_brewer(palette="Dark2") +
  guides(fill = guide_legend(title="API match manual"))  +
  labs(caption="") + 
  scale_x_productlist(labels=c("Not player"="Not player",
          "Match play"="Match play",
          "Off court"="Off court",
          "Crowd"="Crowd",
          "Close-up"="Close-up")) +
  coord_flip() +
  ylab("Proportion of faces") + xlab("Situation") + theme(legend.position = "none")  +
  ggtitle("(B)")

MosSaBg <- ggplot(data=aminFaces) +
   geom_mosaic(aes(x=product(shotangle), fill = bg)) +#, 
            #position = position_fill(reverse = TRUE), 
            #na.rm=TRUE) + 
  scale_x_productlist(labels=c("Birds Eye"="Birds \nEye",
                            "Upward"="Upward",
                            "Level" =
                            "Level"))+
  coord_flip() +
  guides(fill = guide_legend(title="Background")) +
  labs(caption="") +
  ylab("Proportion of faces") + xlab("Angle of face capture") + theme(legend.position = "none") +
  ggtitle("(C)")

MosSitBg <- ggplot(data=aminFaces) +
   geom_mosaic(aes(x=product(situation), fill = bg)) + #, 
            #position = position_fill(reverse = TRUE), 
            #na.rm=TRUE) +
   scale_x_productlist(labels=c("Not player"="Not player",
          "Match play"="Match play",
          "Off court"="Off court",
          "Crowd"="Crowd",
          "Close-up"="Close-up")) +
  coord_flip() +
  guides(fill = guide_legend(title="Background")) +
  labs(caption="") +
  ylab("Proportion of faces") + xlab("Situation") + theme(legend.position = "none") +
  ggtitle("(D)")

g1 <- ggplotGrob(MosSaMm + theme(legend.position="bottom"))$grobs
legend1 <- g1[[which(sapply(g1, function(x) x$name) == "guide-box")]]
lheight1 <- sum(legend1$height)

plots1 <- arrangeGrob(MosSaMm, MosSitMm, ncol=2)
plots1 <- arrangeGrob(plots1, legend1, nrow=2, heights = unit.c(unit(1, "npc") - lheight1, lheight1))
 

g2 <- ggplotGrob(MosSaBg + theme(legend.position="bottom"))$grobs
legend2 <- g2[[which(sapply(g2, function(x) x$name) == "guide-box")]]
lheight2 <- sum(legend2$height)

plots2 <- arrangeGrob(MosSaBg, MosSitBg, ncol=2)
plots2 <- arrangeGrob(plots2, legend2, nrow=2, heights = unit.c(unit(1, "npc") - lheight2, lheight2))

grid.arrange(plots1, plots2, nrow=2)
```

## Image characteristics

Image characteristics (Table \ref{tab:Scene-Attributes-Table}) were constructed during manual tagging. The scene information was tallied and the most common combinations of these features shown in Table \ref{tab:Image-Characteristics-Table}.

Figure \ref{fig:ICFigs} examines the relationships between several of the image characteristics, and face detection by the Google API, using mosaic plots. Plot (A) compares the angle of capture by detection rate. Color is mapped to detection. Most images were taken at a level angle. The highest capture rate occurred when the angle was level, and least with a bird's eye, as might be expected. Plot (B) compares the situation with face detection. Close-up was the most common situation. The detection rate was the same across situations.

The mosaic \ref{fig:ICFigs} (B) in Figure \ref{fig:ICFigs}, shows the amount of Faces captured during each possible situation. It contrasts the proportion of the images captured in each situation that either did or did not match the faces annotated manually. It can be seen that the largest amount of the faces were captured in a close up situation and there were more of these faces that were found by the APIs that did match manually derived faces. This proportion is steady across most situations, it can be inferred the situation may not be influencing the APIs rate of detection.

Plots (C) and (D) show the three way relationship between capture angle, situation, background and face detection. In the level capture angle most faces were detected against the logo wall, but with bird's eye most were captured against the court. In close-ups the most faces were detected against the logo wall.

## Player accessories

The use of accessories like glasses and head wear, visors or hats, is common during the Australian Open as play is mostly outdoors during daytime.  Table \ref{tab:accessories} shows player use of head wear and glasses, as tagged manually. Most players do not have glasses, but head wear, either visors or hats, are very common. 

```{r accessories, eval=TRUE, fig.cap = "\\label{tab:accessories}"}
aminFaces %>%
  filter(type=="Manual") -> ManFaces

#prop.table(table(ManFaces$glasses, ManFaces$visorhat),1)
```


\begin{table}[ht]
\centering
%\parbox{.5\linewidth}{
\caption{Usage of head wear and glasses by players, as manually tagged. Most players did not wear glasses, and roughly half had hats or visors. } 
\label{tab:accessories}
\begin{tabular}{|l|rr|r|}
\hline 
 & No head wear & Head wear & Total\\ 
\hline
Glasses & 92 & 260 & 352\\
No glasses & 981 & 962 & 1943\\\hline
Total & 1073 & 1222 & 2295 \\
\hline
\end{tabular}
\end{table}

```{r glasses, fig.cap = "\\label{tab:glasses}", digit=3}
aminFaces %>% filter(matchesManual==TRUE) -> afm
  
table(afm$visorhat, afm$type) %>% as.matrix() -> glassesTab

glassesdf <- cbind(c(" ", "Animetrics","Google","Microsoft","Skybiometry"),
                   c("Yes", 
  round(glassesTab[2,2]/glassesTab[2,1],2),
  round(glassesTab[2,3]/glassesTab[2,1],2),
  round(glassesTab[2,4]/glassesTab[2,1],2),
  round(glassesTab[2,5]/glassesTab[2,1],2)),
c("No", 
  round(glassesTab[1,2]/glassesTab[1,1],2),
  round(glassesTab[1,3]/glassesTab[1,1],2),
  round(glassesTab[1,4]/glassesTab[1,1],2),
  round(glassesTab[1,5]/glassesTab[1,1],2))
)

# Headwear
table(afm$visorhat, afm$type) %>% as.matrix() -> headwearTab

headweardf <- cbind(c(" ", "Animetrics","Google","Microsoft","Skybiometry"),
                   c("Yes", 
  round(headwearTab[2,2]/headwearTab[2,1],2),
  round(headwearTab[2,3]/headwearTab[2,1],2),
  round(headwearTab[2,4]/headwearTab[2,1],2),
  round(headwearTab[2,5]/headwearTab[2,1],2)),
c("No", 
  round(headwearTab[1,2]/headwearTab[1,1],2),
  round(headwearTab[1,3]/headwearTab[1,1],2),
  round(headwearTab[1,4]/headwearTab[1,1],2),
  round(headwearTab[1,5]/headwearTab[1,1],2))
)
```


\begin{table}[ht]
%\parbox{.5\linewidth}{
\caption{Detection rates of the APIs for different accessories, as measured against the manual tagging. Rate is the proportion of detected with the accessory relative to without. Google has higher detection rates than the other APIs when a player has head wear or glasses. }
\bigskip
\label{tab:glassesheadwear}
\centering
\begin{tabular}{|l|rr|r|rr|r|}  \hline
& \multicolumn{3}{c|}{Head wear}&\multicolumn{3}{c|}{Glasses}\\\cline{2-7}
API & Yes & No & Rate &  Yes & No & Rate \\\hline
Google & 0.48 & 0.69 & 0.70 & 0.43  & 0.60 & 0.72 \\
Microsoft & 0.19 & 0.32 & 0.59 & 0.14  & 0.26 & 0.54 \\
Skybiometry & 0.16  & 0.28 & 0.57  & 0.12 & 0.23 & 0.52 \\
Animetrics & 0.11  & 0.27 & 0.41 & 0.08 & 0.21 & 0.38 \\\hline
\end{tabular}
\end{table}

Table \ref{tab:glassesheadwear} compares the face capture rate of the different APIs for the player accessories, as compared to the manually tagged face attributes. Conditional proportions are calculated two ways: (1) proportion of the manually tagged faces, (2) proportion detected with the accessory relative to without. From the former conditional probability it can be seen that Google detects more faces regardless of the accessory, with slightly higher proportion when the player has no head wear or glasses. From the latter calculation, Google has less difference between the accessory or not detection rate. Overall, Google detects a higher proportion of faces, but they also have better detection rates if faces of players wearing head wear and glasses. Animetrics performed the worst when a player was wearing head wear or glasses. 



# Future Work

This paper has described the results of a survey of commonly available software for face detection when applied to stills from tennis match video. A long term goal is to detect emotional state of the player, and track this through a match, to study the effect on performance. Several of the face recognition APIs also report an emotional state, but our analysis of this data suggests it is unreliable. 

One of the problems encountered with the emotion reporting, was that individual player's faces were repeatedly tagged with one emotional state, which looked more related to the basic facial features of the player. To accurately detect an individual player's emotions would require getting something like a baseline measurement on what their face looks like in a neutral emotional state, and then measuring differences from that. To study emotions and performance, we would recommend creating a training data set similar to the face recognition data, where emotional state is manually tagged. It would also be important to take comprehensive video footage, to track a player throughout the tournament, rather than isolated images. 

The result of the face recognition survey clearly points to a winner: Google's API. It detected a higher proportion of faces under different conditions and with different accessories. 


# Acknowledgements {-}

The authors wish to thank Tennis Australia for access to the data. The analysis was conducted using R[@R], and the following packages: bookdown [@bookdown], descr [@descr], EBImage [@EBImage], ggthemes [@ggthemes], ggmosaic [@ggmosaic], grid [@R], gridExtra [@gridExtra], gtable [@gtable], kfigr [@kfigr], knitcitations [@knitcitations], knitr [@knitr], pander [@pander], readr [@readr], RefManager [@RefManager], tidyverse [@tidyverse], UpSetR [@UpSetR], xtable [@xtable].


# Supplementary material{-}

The APIs, the scripts used to access them, and the resulting data set are contained in the supplementary materials, available at [https://github.com/mvparrot/face-recognition/paper](https://github.com/mvparrot/face-recognition/paper). The APIs evaluated were:

\begin{itemize}
\item Animetrics: Animetrics Face Recognition API, FaceR API by @Animetrics.
\item Google: Google Cloud Vision API [-@Google]
\item Microsoft: Microsoft Azure Cognitive Services Face API, published by @Microsoft.
\item Skybiometry: @Skybiometry the spin off detection and recognition software of Neurotechnology. 
\end{itemize}

Data files provided are:

\begin{itemize}
\item ManualClassifiedFaces.csv contains the data collected on each face by manual tagging.
\item ManualClassifiedScenes.csv contains scene information about each image.
\end{itemize}

The web app for manual tagging, called taipan, is available from https://github.com/srkob1/taipan. It can be installed using the devtools [@devtools] package function `install_github'. 


<div id="refs"></div>


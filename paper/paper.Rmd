---
title: "Detecting Facial Expressions in Video Stills from Professional Tennis Matches"
author: "Stephanie Kobakian, Mitchell O'Hara-Wild, Dianne Cook, Stephanie Kovalchik"
date: "20 September 2017"
output: bookdown::pdf_document2
toc: false
fig_caption: yes
number_sections: false
fontsize: 11pt
documentclass: article
bibliography: references.bib
biblio-style: harvard
link-citations: true
---


```{r cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
#packages:
library(bookdown)
library(pander)
library(ggplot2)
library(knitcitations)
library(RefManageR)
library(readr)
library(knitr)
library(grid)
library(kfigr)
library(UpSetR)
library(descr)
library(xtable)
library(gtable)
library(ggthemes)
library(EBImage)
library(gridExtra)
library(ggmosaic)
library(tidyverse)
library(purrr)
library(lvplot)

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, kfigr.link=TRUE, kfigr.prefix=TRUE, cache=TRUE, fig.env = TRUE, fig.cap=TRUE, fig.height=3)

 
options("citation_format" = "pandoc")

BibOptions(check.entries = FALSE, style = "markdown", bib.style = "alphabetic", cite.style = 'alphabetic')


#csv files:
amin<-read_csv("data/ALLmetaIMGnamed.csv", col_types = cols(type = col_factor(levels = c("Manual", "Animetrics", "Google", "Microsoft", "Skybiometry")))) %>% 
  filter(!duplicates)
SceneAttributes<-read_csv("data/SceneAttributes.csv")
FaceAttributes<-read_csv("data/FaceAttributes.csv")


aminFaces <- amin %>%
  filter(!is.na(type)) %>% 
  mutate(fileID = as.numeric(factor(file))) %>%
  mutate(FaceKey=paste(fileID, boxID, sep="-")) %>%
  mutate(FaceID=paste(fileID, boxID, substring(type, 1,2), sep="-"))


# colour scheme
type.colours <- c(Animetrics =     "#7BCCC4",
                      Google =     "#FC9272",
                      Manual =     "#C994C7",
                      Microsoft =  "#FA9FB5",
                      Skybiometry ="#ADDD8E")
   
#plot images with overlaid boxes
overlayGgplot <- function(imgList, mergeData, matchBox=FALSE, colourScheme = type.colours, legend=TRUE){
  for(i in imgList){
    
    #read in image
    image <- readImage(paste0("figures/", i))
    #used in analyses
    #image <- readImage(paste0("images/", i))
    
    #convert image to a df, add hex value
    image_df <- data.frame(x=rep(1:nrow(image), ncol(image)),
                           y=rep(1:ncol(image),
                                 rep(nrow(image),
                                     ncol(image))),
                           r=as.vector(image[,,1]),
                           g=as.vector(image[,,2]),
                           b=as.vector(image[,,3]))
    image_df$h <- rgb(image_df[,3:5])
    
    #Create the plot of the image
    p <- ggplot() +
      scale_fill_identity() + 
      geom_tile(data=image_df, aes(x,-y, fill=h)) + 
      theme_void()
    
    # Find associated Face Box information for specific image
    faceData <- aminFaces %>% filter(file == i)
    
    if(nrow(faceData) == 0){
      return(p)
    } else{
      
      faceData <- faceData %>%
        mutate(x1 = minX, x2 = minX, x3 = maxX, x4 = maxX, x5 = minX,
               y1 = minY, y2 = maxY, y3 = maxY, y4 = minY, y5 = minY) %>%
        gather(corner, boxPos, x1:y5) %>%
        mutate(posCoord = substr(corner, 1, 1), posID = substr(corner, 2, 2)) %>%
        dplyr::select(-corner) %>% spread(posCoord, boxPos) %>% 
        dplyr::select(file, type, minX, maxX, minY, maxY, FaceKey, FaceID, posID, x, y)
    }
    
  }
  if(matchBox){
    p1 <- p + geom_rect(data=faceData, 
                        aes(xmin=minX, ymin=-maxY, xmax=maxX, ymax=-minY, 
                            color=FaceKey, group=FaceID), fill=NA) +
      guides(colour = "none")
  }
  else {
    pa <- p + geom_rect(data=faceData, 
                        aes(xmin=minX, ymin=-maxY, xmax=maxX, ymax=-minY, 
                            color=type, group=FaceID), fill=NA)  +
      coord_fixed(ratio=0.85) + scale_color_manual(values = colourScheme)
    
    if (legend) p1 <- pa + guides(colour = guide_legend("")) + theme(legend.position = "bottom")
    else p1<- pa + theme(legend.position = "none")
    }
  
  return(p1)
}


# Create graphs of factors in manual proportions
getManualCount <- function(type, count) {
  return(count[type == "Manual"])
}

ggplotProportion <- function(dataset, factorVar){
  factorVar <- deparse(substitute(factorVar))
  dataset <- dataset %>% filter(matchesManual) %>% group_by_(factorVar, "type") %>% summarise(nTotal=n()) %>% group_by_(factorVar) %>% mutate(ManualCount = getManualCount(type, nTotal)) %>%
    mutate(proportion = nTotal/ManualCount) %>% rename_(xvar = factorVar) %>% filter(type!="Manual")
   ggplot(dataset, aes(x=xvar, y=proportion, group = reorder(type, -proportion), fill=type)) + geom_bar(stat = "identity", position = "dodge") +  ylab("Proportion of faces matched") + xlab(factorVar) + scale_colour_manual(values=type.colours)
}


ALLmetaIMGPlayers <- amin %>% filter(detect=="Player")


# Create UpSetR graph
createUpSet <- function(data) {
VennLists<-data %>%
  split(.$type) %>% 
  map(~ .$FaceKey)%>% 
  fromList() %>%
  upset(order.by="freq", nsets=100)
}
```

#Abstract {-}

This paper examines the effectiveness of facial detection APIs on broadcast video stills of Australian Open tennis matches. The goal is to determine the best API to use for face detection of players throughout a match. For training purposes faces were manually tagged in 6406 images, recording the scene characteristics and features of individual faces. This included information regarding accessories, such as headwear or sunglasses being worn. This enables the performance to be assessed based on conditions, and the APIs were evaluated on their success rate at detecting these faces. The possible obstacles of real time implementation were also assessed via the time taken to complete detection within an image.


#Introduction

Many tennis professionals believe that tennis is a game heavily influenced by the mental states of the players. The opportunity for researching this "inner game" presents itself with the hope of improving the performance, and coaching of, tennis players by improving their "mental game". 
By statistically analyzing the faces and expressions of players during a match, insight may be gained about the mental state of a player and the effects changes in the mental state have on the outcome of a match. The aim of this project is to develop methods to collect accurate information about the facial expressions of elite tennis athletes during match play.


Application Programming Interfaces (APIs) have been used to evaluated the performance of several popular facial recognition software, on the still images derived from broadcast videos of elite tennis matches. 
While it is difficult to know the thoughts and feelings of a player during a match, analysts may be able to gain information through results produced by recognition software. This approach to understanding player's emotions during a match differs to previous standards that have used player's recollections after a game to understand their emotions.

Most facial recognition software began with the intention of security applications. There are others, such as Facebook which links images of people with their profiles.
This means the recognition software currently available was not intended to be used in fast paced, elite sport environment.
It was believed that the capabilities of the software could be limited by their intended security and surveillance purposes. @Boston addresses the 'lack of robustness of current tools in unstructured environments' and this survey explores how detection performance is affected by situations that arise in elite tennis matches.


The aims of the present study were to determine the feasibility of using currently available APIs for extracting facial information of players during broadcasts of professional matches by comparing the performance of several popular facial recognition APIs. A limited selection of accessible APIs was chosen based on their ability to produce appropriate and useful facial recognition. The performance was evaluated against manual classifications obtained in an annotation tool developed by the authors. 

# Methodology

In this study any reference to a 'face' is considered to be an area designated manually or by an API as an area that encloses a human face. These may or may not be actual faces.

##Sample and sampling approach

Images from the Australian Open 2016 were provided by Tennis Australia, with goal being that the sample is representative of the video files to be used for future facial recognition analysis:

- 6406 images, 800 $\times$ 450px 


To produce the set of 6406 images, 5 minute segments were taken from 105 video files a still shot of the video was taken at every three seconds, for the length of each segment. The video files used were the broadcasts of the tennis matches shown on the Seven Network during the Australian Open 2016. The sample included an equal amount of females and males singles tennis matches. The rounds of the competition vary so as to not limit the pool of players to only those who progressed, though there was a higher chance of advancing players reappearing. 

The sample included images that contained the faces of many people, such as players, staff on the court and fans in the crowd. All of these faces were included in the manual annotations as they were likely to be found by the software selected. It was decided that including these additional faces would allow better evaluations of the software's capabilities, and provide information to differentiate between players and other faces captured. Therefore the sample was not filtered at this initial stage.

Matches played during the Australian Open are played on a range of courts available at Melbourne Olympic Park. The sample was selected to be representative of the seven courts that have the Hawk Eye technology enabled.


##Software selection

The initial software to be considered was informed by a report that reviewed 'commercial off-the-shelf (COTS) solutions and related patents for face recognition in video surveillance applications' [@Survey, pp.3].
The selection criteria included the availability, speed, feature selection and whether images and/or videos could be presented for detection. The results of Gorodnichy, Granger and Radtke's [-@Survey] report considered processing speed, feature selection techniques, and the ability to perform both still-to-video and video-to-video recognition.

The report outlined that Animetrics [@Animetrics] required an 'image/face proportion should be at least 1:8 and that at least 64 pixels between eyes are required'. 
SkyBiometry [@Skybiometry] is a 'spin-off of Neurotechnology' which was considered by Gorodnichy et al. [-@Survey].
Companies who have recently expanded their API ranges were also considered. This search produced two of the software, Microsoft API [@Microsoft], provided by Microsoft Cognitive Services, and Google Vision API [@Google].
Online demos were used to test viability, Figure \@ref(fig:Trial-Image) depicts Bernard Tomic and the detected areas found.


```{r Trial-Image, fig.cap = "This image of Bernard Tomic was chosen as a trial image to be presented to each of the software before they were included in the research. Each colour represents a different detection source. It was expected that the software would be able to find this face, despite the player facing away from the camera.\\label{fig:Trial-Image}", message=TRUE, warning=TRUE, dev="jpeg", dpi=300}
imagesList0<-as.list("2016_HSA_R01_BTomic_AUS_vs_DIstomin_UZB_MS157_clip.0013.png")
overlayGgplot(imagesList0, aminFaces, matchBox = FALSE)
```



## Manual annotations 


```{r Scene-Attributes-Table, fig.cap ="\\label{tab:Scene-Attributes-Table}"}
knitr::kable(SceneAttributes, format = "latex", longtable=FALSE, booktabs =TRUE, caption = "Characteristics recorded for each image.")
```

<!--
\begin{table}[htp]
\label{tab:Scene-Attributes-Table}
\caption{Image descriptions that are associated with the attributes of each image. }
\centering
\begin{tabular}{lp{12cm}} \hline
Attribute & Choices \\\hline
Graphic & Live Image, 2D Graphic \\
Background & Crowd, Court, Logo Wall, Not Applicable \\
Person & Yes, No \\
Shot Angle & Level, Birds Eye, Upward \\
Situation & Match play, Close up, Not player, Crowd, Off court, Transition\\\hline
\end{tabular}
\end{table}
-->

A web based annotation tool was developed to allow image annotations that would capture the location of areas selected manually, and annotation of attributes for each face, and scene.
Specific information for each face within each scene was collected. To determine which of the, sometimes many, faces in the scene it would be reasonable for software to detect a standard was created for reasonable detection. 
The faces of players were recorded if it showed their face at a minimum of 20 by 20 pixels. The back of the head was not detected as a face by any software, these areas were classified manually but reclassified as other.
Crowd faces were not the intended targets of the recognition however these faces contributed to our understanding of the software. The same face size standard applied to crowd members, but focus was placed on the most prominent faces. For each of these faces, information was collected regarding the attributes in Table \@ref(tab:Face-Attribute-Tables).



```{r Face-Attribute-Tables, results='asis', fig.cap = "\\label{tab:Face-Attribute-Tables}"}
knitr::kable(FaceAttributes, format = "latex", longtable=FALSE, booktabs = TRUE, caption = "Characteristics recorded for each  face. Most were easy to assess for each face, with the exception of obscured and head angle.")
```



The web app was created using the Shiny package for R [@shiny] and has been developed into an R package called taipan, as it is a Tool for Annotating Images in Preparation for ANalysis.
If there was a face in the image the annotator was able to highlight that section of the image to contain the face. This selection activated a set of attributes questions for the annotator to answer, and allowed information to be recorded for the face. The records included the location of the box in the image, and all appropriate answers. 
When a face was not visible in the image only the scene attributes were applicable for that image. All of the annotations for this sample were completed by one annotator to provide a consistent sample of faces annotated manually. The initial decisions of what would be reasonably detected was made by several people.


##Software usage

The face recognition software APIs were accessed via a R [@R] script, calling the APIs relied on the httr [@httr] package. The scripts looped through the images, individually posting a request to retrieve information provided and convert it into an appropriate format for analysis. Special handling was required due to the limits on requests per minute of Skybiometry and Microsoft, time-controlling and error management of the requests was incorporated for these APIs.

Figure \@ref(fig:timeFigure) displays the processing time required by the APIs for each image, as a letter value plot [@letterValue]. A letter value plot is like a box plot, but more suited to skewed data like this. (Both Microsoft and Google have a few images each where they have taken a long time to complete and these were removed from this plot.) The times are quite variable between APIs, with Skybiometry being the most consistently fast. Microsoft and Animetrics are quite variable regardless of the number of faces found. Google is variable in time taken when there are few faces present, and then takes a little longer on average to find faces as the number of them in the image increases. 

```{r timeData}
times <- amin %>% 
  filter(type!="Manual") %>% 
  select(file, type, time.user.self, time.sys.self) %>% 
  group_by(file, type) %>% 
  mutate(n=n(), time.user.self=first(time.user.self), time.sys.self=first(time.sys.self))  
```

```{r timeFigure, fig.height=6, fig.cap="System time taken by the four APIs to finish searching individual images, by number of faces discovered, displayed as a letter value plot.  Skybiometry is the most consistently fast.\\label{fig:timeFigure}"}
# 
# t1 <- ggplot(times) +  
#   geom_smooth(aes(x=n, y=time.sys.self, colour=type), method="lm") + 
#   xlab("Number of faces detected") + ylab("System time (sec)") +  
#   scale_colour_manual("API", values=type.colours) + theme(legend.position = "none")

# t2 <- ggplot(times) +
#   geom_jitter(aes(x=n, y=time.sys.self, colour=type), alpha=0.2, width=0.3, height=0.01) +
#   facet_wrap(~type) +
#   ylim(c(0, 0.1)) +
#   #geom_smooth(aes(x=n, y=time.sys.self, colour=type), method="lm", se=FALSE) +
#   xlab("Number of faces detected") +
#   scale_colour_manual("API", values=type.colours) + theme(legend.position = "none")

#grid.arrange(t1,t2,nrow=1)

# letter value plots
t2 <- ggplot(times, aes(as.factor(n), time.sys.self)) + 
  geom_lv(aes(fill = ..LV..)) +
  #ylim(c(0, 0.1)) + 
  facet_wrap(~type) +
  scale_y_continuous("System time (seconds)", 
    breaks=seq(0, .08, .02), limits=c(0, 0.08)) +
  # somethings funky when try to change axis labels
  xlab("Number of faces detected") 

t2
``` 


```{r eval=FALSE}
times %>% 
  split(.$type) %>%  map_df(~ summary(.$time.user.self, digits=max(2, getOption("digits")-2))) %>%
  rownames_to_column %>%  gather(var, value, -rowname) %>% 
  spread(rowname, value) %>% select(c("API" = `var`,
                            "Min" = `1`,
                        "1st Qrt" = `2`,
                        "Median"  = `3`,
                        "3rd Qrt" = `5`,
                        "Max"     = `6`)) -> usertime
times %>% 
  split(.$type) %>%  map_df(~ summary(.$time.sys.self, digits=max(2, getOption("digits")-2))) %>%
  rownames_to_column %>%  gather(var, value, -rowname) %>% 
  spread(rowname, value) %>% select(c(
                            "Min" = `1`,
                        "1st Qrt" = `2`,
                        "Median"  = `3`,
                        "3rd Qrt" = `5`,
                        "Max"     = `6`)) -> systime

cbind(usertime, systime) -> timetable

knitr::kable(timetable, booktabs = TRUE, longtable=TRUE, caption="This table allows comparisons to be made between the  amount of time taken for each API took to detect faces in each image. Most of the images take less than one second to process. Google had the highest maximum time of 4.936.", format = "latex") #%>%
 # add_header_above(c(" " = 1, "Group 1" = 5, "Group 2" = 5))
```

It should be considered that the time taken may be an issue for 'real time' processing. It would depend on the amount of frames sampled. This also does not account for the time taken to process the information returned.


##Data processing

The data needed for our analysis was organised by source, separate files for each of the four APIs. They contained the information on the location of the faces found in the images and the time taken to find them. Some APIs also provided detailed information such as the estimated head angle, and locations of specific facial features. The manual tagging app resulted in two files, one containing information about the scene, and the other contained information regarding the faces.


##Results

The manual tagging helped to provide a benchmark for faces found by APIs. The
face detection rate indicated what attributes of an image or face were associated with face detection by an API. 
Faces that were found by multiple APIs indicated either obvious facial features were visible or the APIs may be using similar criteria to detect faces. This application allowed for a broader range of angles, backgrounds and accessories that may have impacted detection.
False discoveries were the instances of faces found by APIs that did not intersect with manually tagged faces. Some of these may actually have been faces or reveal sensitivities in the algorithms of the APIs.
The analysis of image characteristics and accessories indicates whether the APIs were viable choices for detecting faces in sports environments. 




### Modeling detection rate

A logistic regression model was used to assess the characteristics of the face and scene that may have affected the detection likelihood of the face. A step-wise variable selection method was used to find variables of significance for any of the APIs. The binary response was whether the face was detected or not, using the manually tagged data as the benchmark, and characteristics included the setting, angle of shot and player, background, lighting and player adornments. Variables that were not included denoted whether there was a graphic or live shot visible, the role of the person whose face was captured and whether or not there was a person in the image.
detect
A separate model was fit using the faces for each API.

```{r RegressionFunctions, cache=FALSE}
# Prepare data
hitmiss <- function(x){
  allType <- c("Animetrics", "Google", "Microsoft", "Skybiometry")
  hit <- allType %in% x$type  
  x[1,] %>%
    dplyr::select(file:visorhat) %>%
    cbind(type = allType, hit = hit)
}


GlmModelCreation <- function(model, data = aminFaces) {
  glmFits <- data %>% 
  split(.$FaceKey) %>% 
  map_df(~ hitmiss(.)) %>% 
  split(.$type) %>% 
  map(~ glm(model, data = dplyr::select(., -type, -file), binomial(link = "logit")))
}


ConvertModel2Table <- function(model){
  model %>%
    summary %>%
    coef %>%
    as.data.frame %>%
    cbind %>%
    rownames_to_column %>%
    cbind %>%
    dplyr::rename(variable = rowname)
}


GlmModelEstimates <- function(model, data = GlmModelCreation(model)){
  glmSummary <- data %>% 
    map(~ ConvertModel2Table(.)) 
  
  glmPlot <- do.call(rbind, Map(cbind, glmSummary, type = names(glmSummary))) 
  return(glmPlot)
}


# SignificancePlot <- function(model, data = GlmModelEstimates(model)) {
#   data %>%
#   mutate(significant = `Pr(>|z|)` < 0.05) %>%
#   ggplot(aes(x=type, y=`Pr(>|z|)`)) +
#   geom_col(aes(fill=significant)) + 
#   facet_wrap(~ variable) +
#   coord_flip()
# }

EstimatesPlot <- function(model, data = GlmModelEstimates(model)) {
  data %>%
  mutate(significant = `Pr(>|z|)` < 0.05) %>%
  ggplot(aes(x=type, y=Estimate)) +
  geom_col(aes(fill=significant)) +
  facet_wrap(~ variable, scales="free_x", labeller=label_wrap_gen(width = 25, multi_line = TRUE)) +
  coord_flip()
}


ModelPlotResults<-function(model, data = GlmModelEstimates(model)){
  ep<-EstimatesPlot(model, data)
  #sp<- SignificancePlot(model, data) 
  grid.arrange(ep)#, sp)
}
```



```{r ModelEstimates, fig.cap ="\\label{tab:ModelEstimates}"}
TypeModel <- hit ~ 0 + 
              shotangle + 
              bg + 
              bg*shotangle + 
              situation + 
              obscured +
              headangle +
              lighting + 
              glasses + 
              visorhat

TypeGlms <- GlmModelCreation(model=TypeModel)
ModelEstimates <- GlmModelEstimates(data=TypeGlms)

#Create estimates columns
ModelEstimates %>% mutate(Estimate = round(Estimate,2)) %>%
select(variable, type, Estimate) %>%
  spread(key=type, value=Estimate) %>%
  select(variable, A.e=Animetrics, G.e=Google, M.e=Microsoft, S.e=Skybiometry)-> ModelEstimatesTypes 

#Create significance columns
ModelEstimates %>% mutate(sig = ifelse(`Pr(>|z|)` <= 0, "***", 
                            ifelse(`Pr(>|z|)` <= 0.001, "** ",
                            ifelse(`Pr(>|z|)` <= 0.010, "*  ",
                            ifelse(`Pr(>|z|)` <= 0.050, ".  ", 
                                                        "   ")))))  %>%
select(variable, type, sig) %>% 
  spread(key=type, value=sig) %>% 
  select(variable, A.s=Animetrics, G.s=Google, M.s=Microsoft, S.s=Skybiometry) -> ModelEstimatesSign 


ModelEstimatesTable <- as.data.frame(cbind(ModelEstimatesTypes, ModelEstimatesSign[,2:5])) %>%
mutate(A = ifelse(A.s=="   ", "-", A.e),
       G = ifelse(G.s=="   ", "-", G.e),
       M = ifelse(M.s=="   ", "-", M.e),
       S = ifelse(S.s=="   ", "-", S.e))%>% 
  # unite(Animetrics, A, A.s, sep="") %>% 
  # unite(Google, G, G.s, sep="") %>% 
  # unite(Microsoft, M, M.s, sep="") %>% 
  # unite(Skybiometry, S, S.s, sep="") %>%
  # select(variable, Animetrics, Google, Microsoft, Skybiometry)
  mutate(` ` = A.s,
         `  ` = G.s,
         `   ` = M.s,
         `    ` = S.s) %>%
   select(variable, A, ` `,
          G, `  `, 
          M, `   `,
          S, `    `)
  

  
knitr::kable(ModelEstimatesTable, format = "latex", align=c("l", 
                                          "r", "l",
                                          "r", "l",
                                          "r", "l",
                                          "r", "l"),
             booktabs = TRUE, longtable=FALSE, caption="Evaluation of factors affecting face recognition for API, as assessed by logistic regression model. Coefficients from the four models shown (significance: ** 0.01 * 0.05 . 0.01 - NS). Most APIs wre affected by similar factors, with glasses, visor or hat, head angle, lighting, shot angle and situation all contributing significantly to face detection.")
```

*** The number of digits should be consistent, e.g. "-1" should be "-1.00". I don't know how to force this with the kale printing, but one way or another we have to.

For the most part, the APIs were all affected by the same factors. Those that negatively affected all APIs were glasses, visor or hat. Background only negatively affected Google's detection rate. Situation had different effects on each API.
Results for Google were most interesting in the shot angle had differing effects on the detection. 


```{r UnlikelyFace, fig.cap = "The face captured manually in this image was not captured by any API. The head angle, shot angle, lighting, wearing the visor and being captured across the court during the match all contribute to the struggle of the APIs.\\label{fig:UnlikelyFace}", dev="jpeg", dpi=300}

predSet <-
aminFaces %>% select(file,glasses, headangle, lighting, obscured, shotangle, situation, visorhat, bg) %>%
  filter(headangle=="Profile" &
         situation=="Match play" &
         lighting=="Shaded" &
         shotangle=="Upward" &
         bg=="Crowd" &
         obscured=="Yes" &
         visorhat=="Yes")

overlayGgplot(predSet$file)
```


```{r Regression, fig.width=7.5, fig.cap = "The figure depicts the change in the probability of a face being detected by the APIs given a certain attribute level associated with the face. Wearing either glasses, or headwear, a visor or a hat, significantly decreased the likelihood of a manually annotated face being detected by the APIs.\\label{fig:Regression}", eval=FALSE}

#unnecessary as table is included
#ModelPlotResults(hit ~ shotangle + bg + bg*shotangle + situation + lighting + glasses + visorhat)

```



```{r GoogleRegression, fig.cap = "Google Regression\\label{fig:GoogleRegression}"}
aminFaces %>% filter(type=="Manual" | type=="Google") %>%
  mutate(isplayer = ifelse(detect=="Player", 1, 0)) -> players

mod1 <- glm(isplayer ~ 0 + 
              shotangle + 
              bg + 
              bg*shotangle + 
              situation + 
              obscured +
              headangle +
              lighting + 
              glasses + 
              visorhat, data=players, family=binomial(link="logit")) 

#drop1(mod1, test="Chisq")
#summary(mod1)
# currently same model as earlier, give two?

```


When forcing no intercept, all variables are significant at the 0 level, and obscured is only significant at the 0.001 level. 



### Detection rates

Table \@ref(tab:facecounts) shows the amount of faces found by each API relative to whether or not the faces were tagged manually. Google has the closest match to the manual tagging. It is interesting to see the high proportion of faces detected by the APIs that were not considered to be faces by manual tagging. 


```{r table}
FacesAPI <- aminFaces %>% 
  filter(!duplicates) %>%
  select(FaceKey, type) %>%
  table() %>%
  as.data.frame() %>%
  spread(key=type, value=Freq) %>%
  mutate(Animetrics = ifelse(Animetrics==0, "No Face", "Face"),
         Google = ifelse(Google==0, "No Face", "Face"),
         Microsoft = ifelse(Microsoft==0, "No Face", "Face"),
         Manual = ifelse(Manual==0, "No Face", "Face"),
         Skybiometry = ifelse(Skybiometry==0, "No Face", "Face")) 

GM <- CrossTable(FacesAPI$Manual, FacesAPI$Google)
MM <- CrossTable(FacesAPI$Manual, FacesAPI$Microsoft)
SM <- CrossTable(FacesAPI$Manual, FacesAPI$Skybiometry)
AM <- CrossTable(FacesAPI$Manual, FacesAPI$Animetrics)
```


\begin{table}[htp]
\centering
\caption{Table of the amount and proportion of faces each API captured, by manual tagging.}
\label{tab:facecounts}
\begin{tabular}{lllll}\\\cline{1-5}
            & \multicolumn{2}{l}{Faces found} & \multicolumn{2}{l}{Non-faces detected as faces} \\\cline{1-5}
            
            & Count        & Proportion       & Count                & Proportion               \\\cline{1-5}
Google      & 1319         & 0.57             & 638                  & 0.49                     \\
Microsoft   & 565          & 0.24             & 505                  & 0.38                     \\
Skybiometry & 493          & 0.22             & 512                  & 0.39                     \\
Animetrics  & 434          & 0.19             & 527                  & 0.40\\ \cline{1-5}
\end{tabular}
\end{table}


The mismatches were manually examined, and showed that the `r amin %>% filter(type=="Google",!matchesManual) %>% nrow()` potential faces found by Google but not tagged manually were actually faces, but were crowd members. They would need to be used, for the long term purpose of the study, to detect and tag players' emotions during a match.

Comparatively, visual inspection of the `r amin %>% filter(type=="Animetrics",!matchesManual) %>% nrow()` potential faces that did not match manually annotated faces showed the Animetrics results contained many potential faces that were unusual areas to be considered faces, this is discussed in the false discoveries section. 


  
```{r FaceUpSet, fig.cap = "\\label{fig:FaceUpset} Faces Per API Combination. The Bar Chart shows the faces that were recognised by multiple API or found manually. The largest group, with 946 faces, is faces only found by Manual annotations. The following group were the 716 faces recognized both Manually and by the Google API. These combinations give insight to the algorithm specifications as some APIs certain faces when others do not."}
createUpSet(aminFaces)
```


Figure \@ref(fig:FaceUpset) is a set visualization to examine the intersection of the APIs, and the manual annotations. This is produced with the UpSetR [@UpSetR] package. The black bubbles below the bars indicate the API combination that corresponds to the bar count. The largest count comes from manual tagging, followed by Google and manual tagging, and then Google. Faces detected by all four API and manual annotations were the fourth largest count. This says that the APIs agreed on about a quarter of the faces from the manually tagged collection. The fifth group was the most surprising -- these were faces captured by Animetrics alone, that were false discoveries.

## False discoveries

Animetrics provided many surprises. Figure \@ref(fig:UnusualImages) shows four images and the captures where non-faces were detected as faces. In image (a) the player's back is detected as a face.
In image (b) the KIA logo was reported as a face. 
In image (c), where logos on the player's and ball boy's shirts were returned as faces. In image (d) where there are a lot of crowd faces, if you look closely one of the detections is actually a fist (to the right on the center).

The other APIs also returned some non-faces, but the most egregious mistakes came from Animetrics.

```{r UnusualImages, fig.cap = "Four images in which Animetrics located unusual faces. \\label{fig:UnusualImages}", dev="jpeg", dpi=300}
i1 <- "2016_CT6_R01_CGarcia_FRA_vs_BStrycova_CZE_WS145_clip.0015.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(a)")
i2 <- "2016_HSA_R03_FDelbonis_ARG_vs_GSimon_FRA_MS302_clip.0072.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(b)")
i3 <- "2016_CT6_R02_ABeck_GER_vs_TBacsinszky_SUI_WS220_clip.0017.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(c)")
i4 <- "2016_SC2_R01_ATomljanovic_AUS_vs_KBondarenko_UKR_WS1112_clip.0053.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(d)")

gridExtra::grid.arrange(i1, i2, i3, i4, nrow=2)
```



```{r Image-Characteristics-Table, fig.cap ="\\label{tab:Image-Characteristics-Table}"}
ImageCharacteristics <- amin %>% filter(type=="Manual") %>% group_by(situation, bg, shotangle, detect) %>% dplyr::summarise(count=n())
ImageCharacteristics<-arrange(ImageCharacteristics, desc(count))
ImageCharacteristics<-ImageCharacteristics[1:5,]
knitr::kable(ImageCharacteristics, booktabs = TRUE, longtable=FALSE, format.args=list(width=8), caption="Combinations of image attibutes that are most common in the image set. The combination of the Logo Wall background and the Level angle are shared by three of the five image combinations.", format = "latex")
```


```{r ICFigs, fig.width = 8, fig.height = 8, fig.cap = "The mosaic plots indicate the proportion of faces given the angle, or situation, that matched faces found manually.\\label{fig:ICFigs}"}

aminFaces$shotangle <- factor(aminFaces$shotangle, 
                              levels = c("Birds Eye","Upward","Level"))
aminFaces$matchesManual <- factor(aminFaces$matchesManual, levels=c(TRUE, FALSE))

MosSaMm <- ggplot(data=aminFaces) + 
  geom_mosaic(aes(x=product(shotangle), fill=matchesManual)) +
  #geom_text(aes(label="test", x=1, y=0.6)) +
  scale_fill_brewer(palette="Dark2") +
  guides(fill = guide_legend(title="API matches manual"))  +
  labs(caption="") + 
  scale_x_productlist(labels=c("Birds Eye"="Birds \nEye",
                            "Upward"="Upward \nAngle",
                            "Level" =
                            "Level")) + coord_flip() +
  ylab("Proportion of faces") + xlab("Angle of face capture") + 
  theme(legend.position = "none") + 
  ggtitle("(a)") 


aminFaces$situation <- factor(aminFaces$situation, levels = c("Not player",
          "Match play",
          "Off court",
          "Crowd",
          "Close-up"))
aminFaces$matchesManual <- factor(aminFaces$matchesManual, levels=c(TRUE, FALSE))
  
MosSitMm <- ggplot(data=aminFaces) + 
  geom_mosaic(aes(x=product(situation), fill=matchesManual)) +
  scale_fill_brewer(palette="Dark2") +
  guides(fill = guide_legend(title="API match manual"))  +
  labs(caption="") + 
  scale_x_productlist(labels=c("Not player"="Not player",
          "Match play"="Match play",
          "Off court"="Off court",
          "Crowd"="Crowd",
          "Close-up"="Close-up")) +
  coord_flip() +
  ylab("Proportion of faces") + xlab("Situation") + theme(legend.position = "none")  +
  ggtitle("(b)")

MosSaBg <- ggplot(data=aminFaces) +
   geom_mosaic(aes(x=product(shotangle), fill = bg)) +#, 
            #position = position_fill(reverse = TRUE), 
            #na.rm=TRUE) + 
  scale_x_productlist(labels=c("Birds Eye"="Birds \nEye",
                            "Upward"="Upward",
                            "Level" =
                            "Level"))+
  coord_flip() +
  guides(fill = guide_legend(title="Background")) +
  labs(caption="") +
  ylab("Proportion of faces") + xlab("Angle of face capture") + theme(legend.position = "none") +
  ggtitle("(c)")

MosSitBg <- ggplot(data=aminFaces) +
   geom_mosaic(aes(x=product(situation), fill = bg)) + #, 
            #position = position_fill(reverse = TRUE), 
            #na.rm=TRUE) +
   scale_x_productlist(labels=c("Not player"="Not player",
          "Match play"="Match play",
          "Off court"="Off court",
          "Crowd"="Crowd",
          "Close-up"="Close-up")) +
  coord_flip() +
  guides(fill = guide_legend(title="Background")) +
  labs(caption="") +
  ylab("Proportion of faces") + xlab("Situation") + theme(legend.position = "none") +
  ggtitle("(d)")

g1 <- ggplotGrob(MosSaMm + theme(legend.position="bottom"))$grobs
legend1 <- g1[[which(sapply(g1, function(x) x$name) == "guide-box")]]
lheight1 <- sum(legend1$height)

plots1 <- arrangeGrob(MosSaMm, MosSitMm, ncol=2)
plots1 <- arrangeGrob(plots1, legend1, nrow=2, heights = unit.c(unit(1, "npc") - lheight1, lheight1))
 

g2 <- ggplotGrob(MosSaBg + theme(legend.position="bottom"))$grobs
legend2 <- g2[[which(sapply(g2, function(x) x$name) == "guide-box")]]
lheight2 <- sum(legend2$height)

plots2 <- arrangeGrob(MosSaBg, MosSitBg, ncol=2)
plots2 <- arrangeGrob(plots2, legend2, nrow=2, heights = unit.c(unit(1, "npc") - lheight2, lheight2))

grid.arrange(plots1, plots2, nrow=2)

```

## Image characteristics

The characteristics of the images were recorded in the manual tagging process and there are uneven amounts of faces with certain image attributes. These relationships can be explored in mosaics for the images that were all considered manually. The scene information was tallied and the combinations of these features were compared to how many potential faces were found with the combination of scene attributes.

This showed that crowd members faces were often recognized which is helpful as it shows a strong ability of Google's algorithm to recognize faces, even when these faces are not the goal of the research. It also allowed for an increase in understanding how attributes of a face or image impact on the API detection.

The colours in the mosaic \@ref(fig:ICFigs) (a) in Figure \@ref(fig:ICFigs) show the proportion of faces that matched those found manually, given the angle the faces were captured from. There is a greater proportion of API faces that matched the faces found manually than those that did not match. However given the face was captured from a birds eye angle there were a lot less faces proportionally that matched those found manually. The most common faces in the set were those captured level to player's faces, these faces were often recognised by the APIs as well.

The mosaic \@ref(fig:ICFigs) (b) in Figure \@ref(fig:ICFigs), shows the amount of Faces captured during each possible situation. It contrasts the proportion of the images captured in each situation that either did or did not match the faces annotated manually. It can be seen that the largest amount of the faces were captured in a close up situation and there were more of these faces that were found by the APIs that did match manually derived faces. This proportion is steady across most situations, it can be inferred the situation may not be influencing the APIs rate of detection.


Figure \@ref(fig:ICFigs) Plot c contains a mosaic which allows consideration of the interaction between two image attributes. It can be seen that the background being the logo wall and the angle of level is common to the highest amount of faces. There are also no images that were taken at an upward angle with the background of the court.

This mosiac, Figure \@ref(fig:ICFigs) Plot d, allows consideration of the interaction between the background of the image captured and the situation that could possibly be occurring. As seen previously, the logo wall is the most common background, but it is never the background of an image of the crowd. The court is the background of an image only when players are captured, this occurs during close ups and while the court is in play.



## Accessories

The use of accessories like glasses and headwear, visors or hats, was considered as the Australian Open takes place on both indoor and outdoor courts. 
It was assumed that outdoor courts would lead to the use of these accessories and these accessories may contribute to the performance of a recognition software.



```{r accessories, eval=TRUE, fig.cap = "\\label{tab:accessories}"}
aminFaces %>%
  filter(type=="Manual") -> ManFaces

#prop.table(table(ManFaces$glasses, ManFaces$visorhat),1)
```


\begin{table}[ht]
\centering
%\parbox{.5\linewidth}{
\caption{Proportions of faces captured manually tagged as wearing headwear given they wore glasses, or did not.} 
\label{tab:accessories}
\begin{tabular}{ccc}
\hline 
 & Head wear & No Head wear\\ 
\hline
Glasses & 0.50 & 0.50 \\
No Glasses & 0.26 & 0.74 \\
\hline
\end{tabular}
\end{table}

Table \@ref(tab:accessories) of the proportions tells that of the faces wearing glasses, there were a similar amount of faces captured with and without headwear. Of those without glasses only 26% wore headwear. This may be influenced by weather and sunlight on the court during Australian Open matches.



```{r glasses, fig.cap = "\\label{tab:glasses}", digit=3}

ManFaces %>% count(glasses) %>%
  tally() -> ManGlasses

aminFaces %>% filter(matchesManual==TRUE) -> afm
  
table(afm$glasses, afm$type) %>% as.matrix() -> glassesTab


glassesdf <- rbind(c(" ", "Animetrics","Google","Microsoft","Skybiometry"),
c("No", 
  round(glassesTab[1,2]/glassesTab[1,1],2),
  round(glassesTab[1,3]/glassesTab[1,1],2),
  round(glassesTab[1,4]/glassesTab[1,1],2),
  round(glassesTab[1,5]/glassesTab[1,1],2)),
c("Yes", 
  round(glassesTab[2,2]/glassesTab[2,1],2),
  round(glassesTab[2,3]/glassesTab[2,1],2),
  round(glassesTab[2,4]/glassesTab[2,1],2),
  round(glassesTab[2,5]/glassesTab[2,1],2)))
```



\begin{table}[ht]
\centering
%\parbox{.5\linewidth}{
\caption{Conditional proportions of faces of players wearing glasses or not, captured by the different software, in comparison to the manually tagged faces. API's unaffected glasses will have similar proportions for both categories. Animetrics had the biggest proportion difference, and thus, most affected. Google is the least affected.} 
\label{tab:glasses}
\begin{tabular}{ccccc}
\hline 
 & Animetrics & Google & Microsoft & Skybiometry\\ 
\hline
Glasses & 0.08 & 0.43 & 0.14 & 0.12 \\
No Glasses & 0.21 & 0.60 & 0.26 & 0.23\\
\hline
\end{tabular}
\end{table}


# Future Work
The long term goal is to better understand how the emotion's felt by a player during a match affect player performance. A long term aim would be to create a program that automated the collection of player emotion data from throughout a match. This information would be presented in a timeline that allowed match performance, in the form of points won, to be aligned with the emotions felt at certain times throughout. 


Considering the images used during our study were stills derived from Broadcast video files, it would be useful to extend further research to deal with the video files directly. The Google Vision API which produced the best recognition in images does not yet have the potential to detect faces and emotions in a video. 


It should also be considered that these are software focused on providing recognition in certain controlled scenarios.
If the study was controlled to focus on certain camera angles that align with the facial angles these security programs are intended to recognize faces in.


Given that Google found many faces that did not match manually annotated face,  manual errors were a possibility. To search for these Taipan could be extended to allow the face identified to be displayed and confirmed this would allow the annotator to confirm manually whether or not these are faces.


Given that certain scene attribute combinations produced more facial recognition than other combinations a limit should be implemented for the sample of images sent to Google Vision API. This would not only reduce cost but also provide a greater level of detail of the emotions felt by a player during a match. To provide a greater level of information at all points in a match it would be beneficial to derive images from a single camera feed. This feed should match the scene attributes that provided the most Google faces.


To undertake sentiment analysis, the faces found in this set of images could be used. Allowing each face a border of pixels, images could be cropped and produce an individual face image that would form the data set for emotion recognition.
Incorporating audio information from the microphones worn by players may assist in sentiment analysis. By including this information differences between certain emotions that may not be able to be found by facial features only.



# Acknowledgements {-}

The authors wish to thank Tennis Australia for access to the data. The analysis was conducted using R, and the following packages: bookdown[@bookdown], descr[@descr], EBImage [@EBImage], ggthemes [@ggthemes], ggmosaic [@ggmosaic], grid [@grid], gridExtra [@gridExtra], gtable [@gtable], kfigr [@kfigr], knitcitations [@knitcitations], knitr [@knitr] , pander [@pander], readr [@readr], RefManager [@RefManager], tidyverse [@tidyverse], UpSetR [@UpSetR], xtable [@xtable].


# Supplementary material{-}

The APIs, the scripts used to access them, and the resulting data set are contained in the supplementary materials, available at [https://github.com/mvparrot/face-recognition/paper](https://github.com/mvparrot/face-recognition/paper). The APIs evaluated were:

- Animetrics: This name used throughout the paper refers to the Animetrics Face Recognition API, FaceR API by @Animetrics.

- Google: @Google Refers to the Google Cloud Vision API

- Microsoft: This has been used to refer to the Microsoft Azure Cognitive Services Face API, published by @Microsoft.

- Skybiometry: @Skybiometry References the spin off detection and recognition software of Neurotechnology. 

Data files provided are:

- ManualClassifiedFaces.csv resulted from the use of the Manual Annotation Application, it contains information about each face identified manually.
- ManualClassifiedScenes.csv also resulted from the use of the app, it contains information about each image.

# References

<div id="refs"></div>


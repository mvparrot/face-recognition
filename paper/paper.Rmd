---
title: "Detecting Facial Expressions in Video Stills from Professional Tennis Matches"
author: "Stephanie Kobakian, Mitchell O'Hara-Wild, Dianne Cook, Stephanie Kovalchik"
date: "20 September 2017"
output: bookdown::pdf_document2
toc: false
fig_caption: yes
number_sections: false
fontsize: 11pt
documentclass: article
bibliography: references.bib
biblio-style: harvard
link-citations: true
---


```{r cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
#packages:
library(bookdown)
library(pander)
library(ggplot2)
library(knitcitations)
library(RefManageR)
library(readr)
library(knitr)
library(grid)
library(kfigr)
library(UpSetR)
library(descr)
library(xtable)
library(gtable)
library(ggthemes)
library(EBImage)
library(gridExtra)
library(ggmosaic)
library(tidyverse)
library(purrr)

knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, kfigr.link=TRUE, kfigr.prefix=TRUE, cache=TRUE, fig.env = TRUE, fig.cap=TRUE, fig.height=3)

 
options("citation_format" = "pandoc")

BibOptions(check.entries = FALSE, style = "markdown", bib.style = "alphabetic", cite.style = 'alphabetic')


#csv files:
amin<-read_csv("data/ALLmetaIMGnamed.csv", col_types = cols(type = col_factor(levels = c("Manual", "Animetrics", "Google", "Microsoft", "Skybiometry")))) %>% 
  filter(!duplicates)
SceneAttributes<-read_csv("data/SceneAttributes.csv")
FaceAttributes<-read_csv("data/FaceAttributes.csv")


aminFaces <- amin %>%
  filter(!is.na(type)) %>% 
  mutate(fileID = as.numeric(factor(file))) %>%
  mutate(FaceKey=paste(fileID, boxID, sep="-")) %>%
  mutate(FaceID=paste(fileID, boxID, substring(type, 1,2), sep="-"))


# colour scheme
type.colours <- c(Animetrics =     "#7BCCC4",
                      Google =     "#FC9272",
                      Manual =     "#C994C7",
                      Microsoft =  "#FA9FB5",
                      Skybiometry ="#ADDD8E")
   
#plot images with overlaid boxes
overlayGgplot <- function(imgList, mergeData, matchBox=FALSE, colourScheme = type.colours, legend=TRUE){
  for(i in imgList){
    
    #read in image
    image <- readImage(paste0("figures/", i))
    #used in analyses
    #image <- readImage(paste0("images/", i))
    
    #convert image to a df, add hex value
    image_df <- data.frame(x=rep(1:nrow(image), ncol(image)),
                           y=rep(1:ncol(image),
                                 rep(nrow(image),
                                     ncol(image))),
                           r=as.vector(image[,,1]),
                           g=as.vector(image[,,2]),
                           b=as.vector(image[,,3]))
    image_df$h <- rgb(image_df[,3:5])
    
    #Create the plot of the image
    p <- ggplot() +
      scale_fill_identity() + 
      geom_tile(data=image_df, aes(x,-y, fill=h)) + 
      theme_void()
    
    # Find associated Face Box information for specific image
    faceData <- aminFaces %>% filter(file == i)
    
    if(nrow(faceData) == 0){
      return(p)
    } else{
      
      faceData <- faceData %>%
        mutate(x1 = minX, x2 = minX, x3 = maxX, x4 = maxX, x5 = minX,
               y1 = minY, y2 = maxY, y3 = maxY, y4 = minY, y5 = minY) %>%
        gather(corner, boxPos, x1:y5) %>%
        mutate(posCoord = substr(corner, 1, 1), posID = substr(corner, 2, 2)) %>%
        dplyr::select(-corner) %>% spread(posCoord, boxPos) %>% 
        dplyr::select(file, type, minX, maxX, minY, maxY, FaceKey, FaceID, posID, x, y)
    }
    
  }
  if(matchBox){
    p1 <- p + geom_rect(data=faceData, 
                        aes(xmin=minX, ymin=-maxY, xmax=maxX, ymax=-minY, 
                            color=FaceKey, group=FaceID), fill=NA) +
      guides(colour = "none")
  }
  else {
    pa <- p + geom_rect(data=faceData, 
                        aes(xmin=minX, ymin=-maxY, xmax=maxX, ymax=-minY, 
                            color=type, group=FaceID), fill=NA)  +
      coord_fixed(ratio=0.85) + scale_color_manual(values = colourScheme)
    
    if (legend) p1 <- pa + guides(colour = guide_legend("")) + theme(legend.position = "bottom")
    else p1<- pa + theme(legend.position = "none")
    }
  
  return(p1)
}


# Create graphs of factors in manual proportions
getManualCount <- function(type, count) {
  return(count[type == "Manual"])
}

ggplotProportion <- function(dataset, factorVar){
  factorVar <- deparse(substitute(factorVar))
  dataset <- dataset %>% filter(matchesManual) %>% group_by_(factorVar, "type") %>% summarise(nTotal=n()) %>% group_by_(factorVar) %>% mutate(ManualCount = getManualCount(type, nTotal)) %>%
    mutate(proportion = nTotal/ManualCount) %>% rename_(xvar = factorVar) %>% filter(type!="Manual")
   ggplot(dataset, aes(x=xvar, y=proportion, group = reorder(type, -proportion), fill=type)) + geom_bar(stat = "identity", position = "dodge") +  ylab("Proportion of faces matched") + xlab(factorVar) + scale_colour_manual(values=type.colours)
}


ALLmetaIMGPlayers <- amin %>% filter(detect=="Player")


# Create UpSetR graph
createUpSet <- function(data) {
VennLists<-data %>%
  split(.$type) %>% 
  map(~ .$FaceKey)%>% 
  fromList() %>%
  upset(order.by="freq", nsets=100)
}


```

#Abstract {-}

This paper examines the effectiveness of facial detection APIs on broadcast video stills of Australian Open tennis matches. The goal is to determine the best API to use for face detection of players throughout a match. For training purposes faces were manually tagged in 6406 images, recording the scene characteristics and features of individual faces. This included information regarding accessories, such as head wear or sunglasses being worn. This enables the performance to be assessed based on conditions, and the APIs were evaluated on their success rate at detecting these faces. The possible obstacles of real time implementation were also assessed via the time taken to complete detection within an image.


#Introduction

Many tennis professionals believe that tennis is a game heavily influenced by the mental states of the players. The opportunity for researching this "inner game" presents itself with the hope of improving the performance, and coaching of, tennis players by improving their "mental game". 
By statistically analyzing the faces and expressions of players during a match, insight may be gained about the mental state of a player and the effects changes in the mental state have on the outcome of a match. The aim of this project is to develop methods to collect accurate information about the facial expressions of elite tennis athletes during match play.


Application Programming Interfaces (APIs) have been used to evaluated the performance of several popular facial recognition software, on the still images derived from broadcast videos of elite tennis matches. 
While it is difficult to know the thoughts and feelings of a player during a match, analysts may be able to gain information through results produced by recognition software. This approach to understanding player's emotions during a match differs to previous standards that have used player's recollections after a game to understand their emotions.

Most facial recognition software began with the intention of security applications. There are others, such as Facebook which links images of people with their profiles.
This means the recognition software currently available was not intended to be used in fast paced, elite sport environment.
It was believed that the capabilities of the software could be limited by their intended security and surveillance purposes. @Boston addresses the 'lack of robustness of current tools in unstructured environments' and this survey explores how detection performance is affected by situations that arise in elite tennis matches.


The aims of the present study were to determine the feasibility of using currently available APIs for extracting facial information of players during broadcasts of professional matches by comparing the performance of several popular facial recognition APIs. A limited selection of accessible APIs was chosen based on their ability to produce appropriate and useful facial recognition. The performance was evaluated against manual classifications obtained in an annotation tool developed by the authors. 

# Methodology

In this study any reference to a 'face' is considered to be an area designated manually or by an API as an area that encloses a human face. These may or may not be actual faces.

##Sample and sampling approach

Images from the Australian Open 2016 were provided by Tennis Australia, with goal being that the sample is representative of the video files to be used for future facial recognition analysis:

- 6406 images, 800 $\times$ 450px 


To produce the set of 6406 images, 5 minute segments were taken from 105 video files a still shot of the video was taken at every three seconds, for the length of each segment. The video files used were the broadcasts of the tennis matches shown on the Seven Network during the Australian Open 2016. The sample included an equal amount of females and males singles tennis matches. The rounds of the competition vary so as to not limit the pool of players to only those who progressed, though there was a higher chance of advancing players reappearing. 

The sample included images that contained the faces of many people, such as players, staff on the court and fans in the crowd. All of these faces were included in the manual annotations as they were likely to be found by the software selected. It was decided that including these additional faces would allow better evaluations of the software's capabilities, and provide information to differentiate between players and other faces captured. Therefore the sample was not filtered at this initial stage.

Matches played during the Australian Open are played on a range of courts available at Melbourne Olympic Park. The sample was selected to be representative of the seven courts that have the Hawk Eye technology enabled.


##Software selection

The initial software to be considered was informed by a report that reviewed 'commercial off-the-shelf (COTS) solutions and related patents for face recognition in video surveillance applications' [@Survey, pp.3].
The selection criteria included the availability, speed, feature selection and whether images and/or videos could be presented for detection. The results of Gorodnichy, Granger and Radtke's [-@Survey] report considered processing speed, feature selection techniques, and the ability to perform both still-to-video and video-to-video recognition.

The report outlined that Animetrics [@Animetrics] required an 'image/face proportion should be at least 1:8 and that at least 64 pixels between eyes are required'. 
SkyBiometry [@Skybiometry] is a 'spin-off of Neurotechnology' which was considered by Gorodnichy et al. [-@Survey].
Companies who have recently expanded their API ranges were also considered. This search produced two of the software, Microsoft API [@Microsoft], provided by Microsoft Cognitive Services, and Google Vision API [@Google].
Online demos were used to test viability, Figure \@ref(fig:Trial-Image) depicts Bernard Tomic and the detected areas found.


```{r Trial-Image, fig.cap = "This image of Bernard Tomic was chosen as a trial image to be presented to each of the software before they were included in the research. Each colour represents a different detection source. It was expected that the software would be able to find this face, despite the player facing away from the camera.\\label{fig:Trial-Image}", message=TRUE, warning=TRUE, dev="jpeg", dpi=300}
imagesList0<-as.list("2016_HSA_R01_BTomic_AUS_vs_DIstomin_UZB_MS157_clip.0013.png")
overlayGgplot(imagesList0, aminFaces, matchBox = FALSE)
```



## Manual annotations 


```{r Scene-Attributes-Table, fig.cap ="\\label{tab:Scene-Attributes-Table}"}
knitr::kable(SceneAttributes, format = "latex", longtable=FALSE, booktabs =TRUE, caption = "Characteristics recorded for each image.")
```

<!--
\begin{table}[htp]
\label{tab:Scene-Attributes-Table}
\caption{Image descriptions that are associated with the attributes of each image. }
\centering
\begin{tabular}{lp{12cm}} \hline
Attribute & Choices \\\hline
Graphic & Live Image, 2D Graphic \\
Background & Crowd, Court, Logo Wall, Not Applicable \\
Person & Yes, No \\
Shot Angle & Level, Birds Eye, Upward \\
Situation & Match play, Close up, Not player, Crowd, Off court, Transition\\\hline
\end{tabular}
\end{table}
-->

A web based annotation tool was developed to allow image annotations that would capture the location of areas selected manually, and annotation of attributes for each face, and scene.
Specific information for each face within each scene was collected. To determine which of the, sometimes many, faces in the scene it would be reasonable for software to detect a standard was created for reasonable detection. 
The faces of players were recorded if it showed their face at a minimum of 20 by 20 pixels. The back of the head was not detected as a face by any software, these areas were classified manually but reclassified as other.
Crowd faces were not the intended targets of the recognition however these faces contributed to our understanding of the software. The same face size standard applied to crowd members, but focus was placed on the most prominent faces. For each of these faces, information was collected regarding the attributes in Table \@ref(tab:Face-Attribute-Tables).



```{r Face-Attribute-Tables, results='asis', fig.cap = "\\label{tab:Face-Attribute-Tables}"}
knitr::kable(FaceAttributes, format = "latex", longtable=FALSE, booktabs = TRUE, caption = "Characteristics recorded for each  face. Most were easy to assess for each face, with the exception of obscured and head angle.")
```



The web app was created using the Shiny package for R [@shiny] and has been developed into an R package called taipan, as it is a Tool for Annotating Images in Preparation for ANalysis.
If there was a face in the image the annotator was able to highlight that section of the image to contain the face. This selection activated a set of attributes questions for the annotator to answer, and allowed information to be recorded for the face. The records included the location of the box in the image, and all appropriate answers. 
When a face was not visible in the image only the scene attributes were applicable for that image. All of the annotations for this sample were completed by one annotator to provide a consistent sample of faces annotated manually. The initial decisions of what would be reasonably detected was made by several people.


##Software usage

The face recognition software APIs were accessed via a R [@R] script, calling the APIs relied on the httr [@httr] package. The scripts looped through the images, individually posting a request to retrieve information provided and convert it into an appropriate format for analysis. Special handling was required due to the limits on requests per minute of Skybiometry and Microsoft, time-controlling and error management of the requests was incorporated for these APIs.

Figure \@ref(fig:timeFigure) shows the distribution of processing time required by the APIs for each image. The violin plot display created by @Violin 'combines the box plot and the density'. A categorical variable splits the data along the x axis, as would be seen in a box plot display. A density plot is then incorporated on the y axis. This is produced by manipulating a density display; switching the axes allows the continuous variable to increase along on the y axis, then the violin shape is created by mirroring the density. The width of the violin will now be twice the length that would be have been measured vertically in a density plot. 


As seen in the facets of Figure \@ref(fig:timeFigure), both Microsoft and Google had few images which took a significant amount of time to complete, and these were removed from this plot due to concerns regarding the network. The times taken to search are quite variable between APIs, with Skybiometry consistently spending less time than other APIs. Microsoft and Google's times vary regardless of the number of faces found. The lines overlaid in each plot represent the smoothed linear model of the amount of faces, it can be seen that all APIs take longer to complete detection of faces in an image when there are more faces. As all APIs had more images where only one or two faces were found it resulted in more variability in the amount of time taken.


```{r timeData}
times <- amin %>% 
  filter(type!="Manual") %>% 
  select(file, type, time.user.self, time.sys.self) %>% 
  group_by(file, type) %>% 
  mutate(n=n()) %>% 
  distinct() %>% ungroup()
```

```{r timeFigure, fig.height=6, fig.cap="System time taken by the four APIs to finish searching individual images, by number of faces discovered, displayed as violins. The width of the violin shows the most common times for the group of images.\\label{fig:timeFigure}"}

# violin plot
t2 <- ggplot(times, aes(x= as.factor(n), time.sys.self, fill=type)) +
  geom_violin(scale = "width", alpha=0.5) + ylim(0,0.08) +
  #geom_jitter(height = 0.005, width = 0.1, alpha=0.5) +
  facet_wrap(~ type) +
  xlab("Amount of faces found") +
  ylab("Time taken per image (Seconds)") + scale_fill_manual("", values=type.colours) +
  geom_smooth(aes(x=n, time.sys.self), method="lm", se=F, colour="black") + 
  scale_colour_manual("", values=type.colours)

t2
``` 


```{r eval=FALSE}
times %>% 
  split(.$type) %>%  map_df(~ summary(.$time.user.self, digits=max(2, getOption("digits")-2))) %>%
  rownames_to_column %>%  gather(var, value, -rowname) %>% 
  spread(rowname, value) %>% select(c("API" = `var`,
                            "Min" = `1`,
                        "1st Qrt" = `2`,
                        "Median"  = `3`,
                        "3rd Qrt" = `5`,
                        "Max"     = `6`)) -> usertime
times %>% 
  split(.$type) %>%  map_df(~ summary(.$time.sys.self, digits=max(2, getOption("digits")-2))) %>%
  rownames_to_column %>%  gather(var, value, -rowname) %>% 
  spread(rowname, value) %>% select(c(
                            "Min" = `1`,
                        "1st Qrt" = `2`,
                        "Median"  = `3`,
                        "3rd Qrt" = `5`,
                        "Max"     = `6`)) -> systime

cbind(usertime, systime) -> timetable

knitr::kable(timetable, booktabs = TRUE, longtable=TRUE, caption="This table allows comparisons to be made between the  amount of time taken for each API took to detect faces in each image. Most of the images take less than one second to process. Google had the highest maximum time of 4.936.", format = "latex") #%>%
 # add_header_above(c(" " = 1, "Group 1" = 5, "Group 2" = 5))
```

It should be considered that the time taken may be an issue for 'real time' processing. It would depend on the amount of frames sampled. This also does not account for the time taken to process the information returned.


##Data processing

The data needed for our analysis was organised by source, separate files for each of the four APIs. They contained the information on the location of the faces found in the images, and the time taken to find them. Some APIs also provided detailed information such as the estimated head angle, and locations of specific facial features. The manual tagging app resulted in two files, one containing information about the scene, and the other contained information regarding the faces.

The location and size of the boxes around the faces were recorded, these values were used to see if a particular identified face box matched a manually identified face, or a region found by another software. Every face found in an image manually, or by any API was compared. From their locations the intersecting area between them was found. When the ratio of this intersecting area to total area was greater than 0.1 the IDs were altered to reflect the two faces would be considered the same face. This allowed comparison of the identification areas on the same face, as well as contrast the identified faces of each software.

##Results

The manual tagging helped to provide a benchmark for faces found by APIs. The
face detection rate indicated what attributes of an image or face were associated with face detection by an API. 
Faces that were found by multiple APIs indicated either obvious facial features were visible or the APIs may be using similar criteria to detect faces. This application allowed for a broader range of angles, backgrounds and accessories that may have impacted detection.
False discoveries were the instances of faces found by APIs that did not intersect with manually tagged faces. Some of these may actually have been faces or reveal sensitivities in the algorithms of the APIs.
The analysis of image characteristics and accessories indicates whether the APIs were viable choices for detecting faces in sports environments. 




### Modeling detection rate

A logistic regression model was used to assess the characteristics of the face and scene that may have affected the detection likelihood of the face. A step-wise variable selection method was used to find variables of significance for any of the APIs. The binary response was whether the face was detected or not, using the manually tagged data as the benchmark, and characteristics included the setting, angle of shot and player, background, lighting and player adornments. Variables that were not included denoted whether there was a graphic or live shot visible, the role of the person whose face was captured and whether or not there was a person in the image.
detect
A separate model was fit using the faces for each API.

```{r RegressionFunctions, cache=FALSE}
# Prepare data
hitmiss <- function(x){
  allType <- c("Animetrics", "Google", "Microsoft", "Skybiometry")
  hit <- allType %in% x$type  
  x[1,] %>%
    dplyr::select(file:visorhat) %>%
    cbind(type = allType, hit = hit)
}


GlmModelCreation <- function(model, data = aminFaces) {
  glmFits <- data %>% 
  split(.$FaceKey) %>% 
  map_df(~ hitmiss(.)) %>% 
  split(.$type) %>% 
  map(~ glm(model, data = dplyr::select(., -type, -file), binomial(link = "logit")))
}


ConvertModel2Table <- function(model){
  model %>%
    summary %>%
    coef %>%
    as.data.frame %>%
    cbind %>%
    rownames_to_column %>%
    cbind %>%
    dplyr::rename(variable = rowname)
}


GlmModelEstimates <- function(model, data = GlmModelCreation(model)){
  glmSummary <- data %>% 
    map(~ ConvertModel2Table(.)) 
  
  glmPlot <- do.call(rbind, Map(cbind, glmSummary, type = names(glmSummary))) 
  return(glmPlot)
}


# SignificancePlot <- function(model, data = GlmModelEstimates(model)) {
#   data %>%
#   mutate(significant = `Pr(>|z|)` < 0.05) %>%
#   ggplot(aes(x=type, y=`Pr(>|z|)`)) +
#   geom_col(aes(fill=significant)) + 
#   facet_wrap(~ variable) +
#   coord_flip()
# }

EstimatesPlot <- function(model, data = GlmModelEstimates(model)) {
  data %>%
  mutate(significant = `Pr(>|z|)` < 0.05) %>%
  ggplot(aes(x=type, y=Estimate)) +
  geom_col(aes(fill=significant)) +
  facet_wrap(~ variable, scales="free_x", labeller=label_wrap_gen(width = 25, multi_line = TRUE)) +
  coord_flip()
}


ModelPlotResults<-function(model, data = GlmModelEstimates(model)){
  ep<-EstimatesPlot(model, data)
  #sp<- SignificancePlot(model, data) 
  grid.arrange(ep)#, sp)
}
```



```{r ModelEstimates, fig.cap ="\\label{tab:ModelEstimates}"}
TypeModel <- hit ~ 0 + 
              shotangle + 
              bg + 
              bg*shotangle + 
              situation + 
              obscured +
              headangle +
              lighting + 
              glasses + 
              visorhat

TypeGlms <- GlmModelCreation(model=TypeModel)
ModelEstimates <- GlmModelEstimates(data=TypeGlms)

#Create estimates columns
ModelEstimates %>% mutate(Estimate = round(Estimate,2)) %>%
select(variable, type, Estimate) %>%
  spread(key=type, value=Estimate) %>%
  select(variable, A.e=Animetrics, G.e=Google, M.e=Microsoft, S.e=Skybiometry)-> ModelEstimatesTypes 

#Create significance columns
ModelEstimates %>% mutate(sig = ifelse(`Pr(>|z|)` <= 0, "***", 
                            ifelse(`Pr(>|z|)` <= 0.001, "** ",
                            ifelse(`Pr(>|z|)` <= 0.010, "*  ",
                            ifelse(`Pr(>|z|)` <= 0.050, ".  ", 
                                                        "   ")))))  %>%
select(variable, type, sig) %>% 
  spread(key=type, value=sig) %>% 
  select(variable, A.s=Animetrics, G.s=Google, M.s=Microsoft, S.s=Skybiometry) -> ModelEstimatesSign 


ModelEstimatesTable <- as.data.frame(cbind((ModelEstimatesTypes), ModelEstimatesSign[,2:5])) %>%
mutate(A = ifelse(A.s=="   ", "-", format(round(A.e, 2), nsmall = 2)),
       G = ifelse(G.s=="   ", "-", format(round(G.e, 2), nsmall = 2)),
       M = ifelse(M.s=="   ", "-", format(round(M.e, 2), nsmall = 2)),
       S = ifelse(S.s=="   ", "-", format(round(S.e, 2), nsmall = 2))) %>% 
  select(variable, A, ` ` = A.s,
         G, `  ` = G.s,
         M, `   ` = M.s,
         S, `    ` = S.s)

  
knitr::kable(ModelEstimatesTable, format = "latex", align=c('l', 
                                           'r', 'l',
                                           'r', 'l',
                                           'r', 'l',
                                           'r', 'l'),
             booktabs = TRUE, longtable=FALSE, caption="Evaluation of factors affecting face recognition for API, as assessed by logistic regression model. Coefficients from the four models shown (significance: ** 0.01 * 0.05 . 0.01 - NS). Most APIs wre affected by similar factors, with glasses, visor or hat, head angle, lighting, shot angle and situation all contributing significantly to face detection.")
```


For the most part, the APIs were all affected by the same factors. Those that negatively affected all APIs were glasses, visor or hat. Background only negatively affected Google's detection rate. Situation had different effects on each API.
Results for Google were most interesting in the shot angle had differing effects on the detection. 


```{r UnlikelyFace, fig.cap = "The face captured manually in this image was not captured by any API. The head angle, shot angle, lighting, wearing the visor and being captured across the court during the match all contribute to the struggle of the APIs.\\label{fig:UnlikelyFace}", dev="jpeg", dpi=300}

predSet <-
aminFaces %>% select(file,glasses, headangle, lighting, obscured, shotangle, situation, visorhat, bg) %>%
  filter(headangle=="Profile" &
         situation=="Match play" &
         lighting=="Shaded" &
         shotangle=="Upward" &
         bg=="Crowd" &
         obscured=="Yes" &
         visorhat=="Yes")

overlayGgplot(predSet$file)
```


```{r Regression, fig.width=7.5, fig.cap = "The figure depicts the change in the probability of a face being detected by the APIs given a certain attribute level associated with the face. Wearing either glasses, or headwear, a visor or a hat, significantly decreased the likelihood of a manually annotated face being detected by the APIs.\\label{fig:Regression}", eval=FALSE}

#unnecessary as table is included
#ModelPlotResults(hit ~ shotangle + bg + bg*shotangle + situation + lighting + glasses + visorhat)

```



```{r GoogleRegression, fig.cap = "Google Regression\\label{fig:GoogleRegression}"}
aminFaces %>% filter(type=="Manual" | type=="Google") %>%
  mutate(isplayer = ifelse(detect=="Player", 1, 0)) -> players

mod1 <- glm(isplayer ~ 0 + 
              shotangle + 
              bg + 
              bg*shotangle + 
              situation + 
              obscured +
              headangle +
              lighting + 
              glasses + 
              visorhat, data=players, family=binomial(link="logit")) 

#drop1(mod1, test="Chisq")
#summary(mod1)
# currently same model as earlier, give two?

```


When forcing no intercept, all variables are significant at the 0 level, and obscured is only significant at the 0.001 level. 



### Detection rates and API comparison

Each API returned areas in the image that indicated the face location. These are defined using the four points, marking the four corners of a rectangle.  

```{r FaceData, eval=FALSE, fig.cap = "\\label{fig:FaceData}  The bar chart of the number of faces detected by each API. Google's Facial Recognition API recognized almost 1000 more faces than the next best API, Microsoft."}

ggplot(amin, aes(x = type, fill=type)) + 
  geom_bar(position="dodge") +
  xlab("Facial Recognition APIs") +
  scale_x_discrete(limits=c("Manual","Google","Microsoft","Skybiometry","Animetrics")) +
  ylab("Number of Faces") + 
  guides(fill=FALSE) +
  labs(caption="") + scale_fill_manual(values=type.colours)
```

Table \@ref(tab:facecounts) displays the proportion of the manually tagged faces that the APIs detected. Google had by far the best detection rate. In addition, the APIs also detected non-faces, in images that produced no manually tagged faces. Google had the highest of these, and the detections were for the most part helpful, primarily of tiny faces in the crowd, that were ignored by manual tagging. They would not be needed for the long term purpose of the study, to detect and tag players' emotions during a match.

In comparison, examining the `r amin %>% filter(type=="Animetrics",!matchesManual) %>% nrow()` images in which Animetrics detected faces, not reported manually, revealed surprising results, which are discussed in more detail later.


```{r table}
FacesAPI <- aminFaces %>% 
  filter(!duplicates) %>%
  select(FaceKey, type) %>%
  table() %>%
  as.data.frame() %>%
  spread(key=type, value=Freq) %>%
  mutate(Animetrics = ifelse(Animetrics==0, "No Face", "Face"),
         Google = ifelse(Google==0, "No Face", "Face"),
         Microsoft = ifelse(Microsoft==0, "No Face", "Face"),
         Manual = ifelse(Manual==0, "No Face", "Face"),
         Skybiometry = ifelse(Skybiometry==0, "No Face", "Face")) 

GM <- CrossTable(FacesAPI$Manual, FacesAPI$Google)
MM <- CrossTable(FacesAPI$Manual, FacesAPI$Microsoft)
SM <- CrossTable(FacesAPI$Manual, FacesAPI$Skybiometry)
AM <- CrossTable(FacesAPI$Manual, FacesAPI$Animetrics)
```

\begin{table}[htp]
%\parbox{.5\linewidth}{
\caption{Proportion of API matches with the 2295 manually tagged faces, and faces detected in the 1314 images with no manually tagged faces.}
\bigskip
\label{tab:facecounts}
\centering
\begin{tabular}{|l|rr|rr|}  \hline
& \multicolumn{2}{r|}{Faces found}&\multicolumn{2}{r|}{Non-faces found}\\\cline{2-5}
API & Count & Prop & Count & Prop \\\hline
Google & 1319 & 0.57 & 638 & 0.49 \\
Microsoft & 565 & 0.25 & 505 & 0.38 \\
Skybiometry & 493 & 0.21 & 512 & 0.39 \\
Animetrics & 434 & 0.19 & 527 & 0.40 \\\hline
\end{tabular}
\end{table}
  
```{r FaceUpSet, fig.cap = "\\label{fig:FaceUpset} Set visualization, to examine intersections between API and manual face detections:  bar charts show counts, by API (lower left) and for combinations (main part of plot), and dot display (bottom) indicates the group intersections. All five way intersections are examined, but only the first few have large counts. The largest group is the manual tagging with 946 faces uniquely identified. The intersection between manual tagging and Google is 716 faces. The intersection of all four APIs and manual tagging is fourth highest, at 342."}
setdata <- aminFaces %>% 
  mutate(count = 1) %>%
  select(type, FaceID, count) %>%
  separate(FaceID, c("id1", "id2", "API")) %>%
  unite(id, c("id1", "id2")) %>%
  spread(API, count, fill=0) %>% 
  arrange(id) %>% 
  group_by(id) %>%
  summarise(Animetrics=as.integer(sum(An)), 
            Google=as.integer(sum(Go)), 
            Manual=as.integer(sum(Ma)),
            Microsoft=as.integer(sum(Mi)), 
            Skybiometry=as.integer(sum(Sk))) 
  
setdata <- setdata %>% mutate(id=factor(id)) 
setdata <- as.data.frame(setdata)

upset(setdata, order.by="freq")
# colsum <- apply(setdata[,-1], 2, sum)
# rowsum <- apply(setdata[,-1], 1, sum)
# onlyone <- setdata[rowsum==1,] 
# apply(onlyone[,-1], 2, sum)
```


Figure \@ref(fig:FaceUpset) is a set visualization to examine the intersection of the APIs, and the manual annotations. This is produced with the UpSetR [@UpSetR] package. The black bubbles below the bars indicate the API combination that corresponds to the bar count. The largest count comes from manual tagging, followed by Google and manual tagging, and then Google. Faces detected by all four API and manual annotations were the fourth largest count. This says that the APIs agreed on about a quarter of the faces from the manually tagged collection. The fifth group was the most surprising -- these were faces captured by Animetrics alone, that were false discoveries.

## False discoveries

Animetrics provided many surprises. Figure \@ref(fig:UnusualImages) shows four images and the captures where non-faces were detected as faces. In image (a) the player's back is detected as a face.
In image (b) the KIA logo was reported as a face. 
In image (c), where logos on the player's and ball boy's shirts were returned as faces. In image (d) where there are a lot of crowd faces, if you look closely one of the detections is actually a fist (to the right on the center).

The other APIs also returned some non-faces, but the most egregious mistakes came from Animetrics.

```{r UnusualImages, fig.cap = "Four images in which Animetrics located unusual faces. \\label{fig:UnusualImages}", dev="jpeg", dpi=300}
i1 <- "2016_CT6_R01_CGarcia_FRA_vs_BStrycova_CZE_WS145_clip.0015.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(a)")
i2 <- "2016_HSA_R03_FDelbonis_ARG_vs_GSimon_FRA_MS302_clip.0072.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(b)")
i3 <- "2016_CT6_R02_ABeck_GER_vs_TBacsinszky_SUI_WS220_clip.0017.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(c)")
i4 <- "2016_SC2_R01_ATomljanovic_AUS_vs_KBondarenko_UKR_WS1112_clip.0053.png" %>% overlayGgplot(., legend=FALSE) + ggtitle("(d)")

gridExtra::grid.arrange(i1, i2, i3, i4, nrow=2)
```



```{r Image-Characteristics-Table, fig.cap ="\\label{tab:Image-Characteristics-Table}"}
ImageCharacteristics <- amin %>% filter(type=="Manual") %>% group_by(situation, bg, shotangle, detect) %>% dplyr::summarise(count=n())
ImageCharacteristics<-arrange(ImageCharacteristics, desc(count))
ImageCharacteristics<-ImageCharacteristics[1:5,]
knitr::kable(ImageCharacteristics, booktabs = TRUE, longtable=FALSE, format.args=list(width=8), caption="Top five most frequent image attribute combinations.", format = "latex")
```


```{r ICFigs, fig.width = 8, fig.height = 8, fig.cap = "Relationships between court and face characteristics, and API detection, using mosaic plots: (a) Angle of face capture, (b) court situation, (c) angle of capture by background, and (d) situtation of capture by background. Most face captures occurred at a level angle, and this produces slightly better detection. Most situtation face captures are close up, but the detection rate is identical regardless of situation. Most level angles had the logo wall in the background, while most birds-eye had the court as background, as we would expect. Close ups were mostly against the logo wall. \\label{fig:ICFigs}"}

aminFaces$shotangle <- factor(aminFaces$shotangle, 
                              levels = c("Birds Eye","Upward","Level"))
aminFaces$matchesManual <- factor(aminFaces$matchesManual, levels=c(TRUE, FALSE))

MosSaMm <- ggplot(data=aminFaces) + 
  geom_mosaic(aes(x=product(shotangle), fill=matchesManual)) +
  #geom_text(aes(label="test", x=1, y=0.6)) +
  scale_fill_brewer(palette="Dark2") +
  guides(fill = guide_legend(title="API matches manual"))  +
  labs(caption="") + 
  scale_x_productlist(labels=c("Birds Eye"="Birds \nEye",
                            "Upward"="Upward \nAngle",
                            "Level" =
                            "Level")) + coord_flip() +
  ylab("Proportion of faces") + xlab("Angle of face capture") + 
  theme(legend.position = "none") + 
  ggtitle("(a)") 


aminFaces$situation <- factor(aminFaces$situation, levels = c("Not player",
          "Match play",
          "Off court",
          "Crowd",
          "Close-up"))
aminFaces$matchesManual <- factor(aminFaces$matchesManual, levels=c(TRUE, FALSE))
  
MosSitMm <- ggplot(data=aminFaces) + 
  geom_mosaic(aes(x=product(situation), fill=matchesManual)) +
  scale_fill_brewer(palette="Dark2") +
  guides(fill = guide_legend(title="API match manual"))  +
  labs(caption="") + 
  scale_x_productlist(labels=c("Not player"="Not player",
          "Match play"="Match play",
          "Off court"="Off court",
          "Crowd"="Crowd",
          "Close-up"="Close-up")) +
  coord_flip() +
  ylab("Proportion of faces") + xlab("Situation") + theme(legend.position = "none")  +
  ggtitle("(b)")

MosSaBg <- ggplot(data=aminFaces) +
   geom_mosaic(aes(x=product(shotangle), fill = bg)) +#, 
            #position = position_fill(reverse = TRUE), 
            #na.rm=TRUE) + 
  scale_x_productlist(labels=c("Birds Eye"="Birds \nEye",
                            "Upward"="Upward",
                            "Level" =
                            "Level"))+
  coord_flip() +
  guides(fill = guide_legend(title="Background")) +
  labs(caption="") +
  ylab("Proportion of faces") + xlab("Angle of face capture") + theme(legend.position = "none") +
  ggtitle("(c)")

MosSitBg <- ggplot(data=aminFaces) +
   geom_mosaic(aes(x=product(situation), fill = bg)) + #, 
            #position = position_fill(reverse = TRUE), 
            #na.rm=TRUE) +
   scale_x_productlist(labels=c("Not player"="Not player",
          "Match play"="Match play",
          "Off court"="Off court",
          "Crowd"="Crowd",
          "Close-up"="Close-up")) +
  coord_flip() +
  guides(fill = guide_legend(title="Background")) +
  labs(caption="") +
  ylab("Proportion of faces") + xlab("Situation") + theme(legend.position = "none") +
  ggtitle("(d)")

g1 <- ggplotGrob(MosSaMm + theme(legend.position="bottom"))$grobs
legend1 <- g1[[which(sapply(g1, function(x) x$name) == "guide-box")]]
lheight1 <- sum(legend1$height)

plots1 <- arrangeGrob(MosSaMm, MosSitMm, ncol=2)
plots1 <- arrangeGrob(plots1, legend1, nrow=2, heights = unit.c(unit(1, "npc") - lheight1, lheight1))
 

g2 <- ggplotGrob(MosSaBg + theme(legend.position="bottom"))$grobs
legend2 <- g2[[which(sapply(g2, function(x) x$name) == "guide-box")]]
lheight2 <- sum(legend2$height)

plots2 <- arrangeGrob(MosSaBg, MosSitBg, ncol=2)
plots2 <- arrangeGrob(plots2, legend2, nrow=2, heights = unit.c(unit(1, "npc") - lheight2, lheight2))

grid.arrange(plots1, plots2, nrow=2)

```

## Image characteristics

Image characteristics (Table \@ref(tab:Scene-Attributes-Table)) were constructed during manual tagging. The scene information was tallied and the most common combinations of these features shown in Table \@ref(tab:Image-Characteristics-Table).

Figure \@ref(fig:ICFigs) examines the relationships between several of the image characteristics, and face detection by the Google API, using mosaic plots. Plot (a) compares the angle of capture by detection rate. Colour is mapped to detection. Most images were taken at a level angle. The highest capture rate occurred when the angle was level, and least with a bird's eye, as might be expected. Plot (b) compares the situation with face detection. Close-up was the most common situation. The detection rate was the same across situations.

The mosaic \@ref(fig:ICFigs) (b) in Figure \@ref(fig:ICFigs), shows the amount of Faces captured during each possible situation. It contrasts the proportion of the images captured in each situation that either did or did not match the faces annotated manually. It can be seen that the largest amount of the faces were captured in a close up situation and there were more of these faces that were found by the APIs that did match manually derived faces. This proportion is steady across most situations, it can be inferred the situation may not be influencing the APIs rate of detection.

Plots (c) and (d) show the three way relationship between capture angle, situation, background and face detection. In the level capture angle most faces were detected against the logo wall, but with bird's eye most were captured against the court. In close-ups the most faces were detected against the logo wall.

## Player accessories

The use of accessories like glasses and head wear, visors or hats, is common during the Australian Open as play is mostly outdoors during daytime.  Table \@ref(tab:accessories) shows player use of head wear and glasses, as tagged manually. Table \@ref(tab:glasses) compares the face capture rate of the different APIs for the player acessories.

of the proportions tells that of the faces wearing glasses, there were a similar amount of faces captured with and without head wear. Of those without glasses only 26% wore head wear. This may be influenced by weather and sunlight on the court during Australian Open matches.


```{r accessories, eval=TRUE, fig.cap = "\\label{tab:accessories}"}
aminFaces %>%
  filter(type=="Manual") -> ManFaces

#prop.table(table(ManFaces$glasses, ManFaces$visorhat),1)
```


\begin{table}[ht]
\centering
%\parbox{.5\linewidth}{
\caption{Usage of head wear and glasses by players. Most players did not wear glasses, and roughly half had hats or visors. } 
\label{tab:accessories}
\begin{tabular}{|l|rr|r|}
\hline 
 & No head wear & Head wear & Total\\ 
\hline
Glasses & 92 & 260 & 352\\
No glasses & 981 & 962 & 1943\\\hline
Total & 1073 & 1222 & 2295 \\
\hline
\end{tabular}
\end{table}


```{r glasses, fig.cap = "\\label{tab:glasses}", digit=3}
aminFaces %>% filter(matchesManual==TRUE) -> afm
  
table(afm$visorhat, afm$type) %>% as.matrix() -> glassesTab

glassesdf <- cbind(c(" ", "Animetrics","Google","Microsoft","Skybiometry"),
                   c("Yes", 
  round(glassesTab[2,2]/glassesTab[2,1],2),
  round(glassesTab[2,3]/glassesTab[2,1],2),
  round(glassesTab[2,4]/glassesTab[2,1],2),
  round(glassesTab[2,5]/glassesTab[2,1],2)),
c("No", 
  round(glassesTab[1,2]/glassesTab[1,1],2),
  round(glassesTab[1,3]/glassesTab[1,1],2),
  round(glassesTab[1,4]/glassesTab[1,1],2),
  round(glassesTab[1,5]/glassesTab[1,1],2))
)

# Headwear
table(afm$visorhat, afm$type) %>% as.matrix() -> headwearTab

headweardf <- cbind(c(" ", "Animetrics","Google","Microsoft","Skybiometry"),
                   c("Yes", 
  round(headwearTab[2,2]/headwearTab[2,1],2),
  round(headwearTab[2,3]/headwearTab[2,1],2),
  round(headwearTab[2,4]/headwearTab[2,1],2),
  round(headwearTab[2,5]/headwearTab[2,1],2)),
c("No", 
  round(headwearTab[1,2]/headwearTab[1,1],2),
  round(headwearTab[1,3]/headwearTab[1,1],2),
  round(headwearTab[1,4]/headwearTab[1,1],2),
  round(headwearTab[1,5]/headwearTab[1,1],2))
)
```


\begin{table}[ht]
%\parbox{.5\linewidth}{
\caption{Conditional proportions of faces of players wearing glasses or headwear or not, captured by the different software, as a proportion of the manually tagged faces. API's will have similar proportions for both categories if they are not affected by the presence of an accessory. Animetrics had the biggest proportion difference, and thus, most affected. Google is the least affected.}
\bigskip
\label{tab:glassesheadwear}
\centering
\begin{tabular}{|l|rr|rr|}  \hline
& \multicolumn{2}{r|}{Headwear}&\multicolumn{2}{r|}{Glasses}\\\cline{2-5}
API & Yes & No & Yes & No \\\hline
Google & 0.48 & 0.69 & 0.43 & 0.60 \\
Microsoft & 0.19 & 0.32 & 0.14 & 0.26 \\
Skybiometry & 0.16 & 0.28 & 0.12 & 0.23 \\
Animetrics & 0.11 & 0.27 & 0.08 & 0.21 \\\hline
\end{tabular}
\end{table}

The conditional proportions are interpreted as the proportion of faces found by the API, that were tagged manually and having the attributes that were tabulated. For example, the value in the first row and first column of table \@ref(tab:glassesheadwear) highlights that Google found 0.48, or 48%, of the faces found manually and tagged as wearing headwear. As the values in the row for Google are the largest amounts in each column, it found more of the manually tagged faces in any situation considered. The next best software, Microsoft, found less than half the amount of manually tagged faces that Google located.

Animetrics may have been impeded by the use of accessories as it located only 0.08, or 8%, of the faces tagged manually as wearing a hat or visor, and 0.11, or 11%, of the faces tagged manually as wearing glasses.
When accessories are not worn, the proportion of faces found are closer to the next best API, Skybiometry.

# Future Work
The long term goal is to better understand how the emotion's felt by a player during a match affect player performance. A long term aim would be to create a program that automated the collection of player emotion data from throughout a match. This information would be presented in a timeline that allowed match performance, in the form of points won, to be aligned with the emotions felt at certain times throughout. 


Considering the images used during our study were stills derived from broadcast video files, it would be useful to extend further research to deal with the video files directly. The Google Vision API which produced the best recognition in images does not yet have the potential to detect faces and emotions in a video. 


It should also be considered that these are software focused on providing recognition in certain controlled scenarios.
If the study was controlled to focus on certain camera angles that align with the facial angles these security programs are intended to recognize faces in.


Google found many faces that did not match manually annotated faces, this showed manual errors were a possibility. To search for these Taipan could be extended to allow the face identified to be displayed and confirmed this would allow the annotator to confirm manually whether or not these are faces.


Given that certain scene attribute combinations produced more successful facial recognition than other combinations, a limit should be implemented for the sample of images sent to Google Vision API. This would not help to reduce cost. To provide a greater level of information at all points in a match it would be beneficial to derive images from a single camera feed. This feed should match the scene attributes that provided was most successful for Google to return faces.


To undertake sentiment analysis, the faces found in this set of images could be used. Allowing each face a border of pixels, images could be cropped and produce an individual face image that would form the data set for emotion recognition.
Incorporating audio information from the microphones worn by players may assist in sentiment analysis. By including this information differences between certain emotions that may not be able to be found by facial features only.



# Acknowledgements {-}

The authors wish to thank Tennis Australia for access to the data. The analysis was conducted using R, and the following packages: bookdown[@bookdown], descr[@descr], EBImage [@EBImage], ggthemes [@ggthemes], ggmosaic [@ggmosaic], grid [@grid], gridExtra [@gridExtra], gtable [@gtable], kfigr [@kfigr], knitcitations [@knitcitations], knitr [@knitr] , pander [@pander], readr [@readr], RefManager [@RefManager], tidyverse [@tidyverse], UpSetR [@UpSetR], xtable [@xtable].


# Supplementary material{-}

The APIs, the scripts used to access them, and the resulting data set are contained in the supplementary materials, available at [https://github.com/mvparrot/face-recognition/paper](https://github.com/mvparrot/face-recognition/paper). The APIs evaluated were:

- Animetrics: Animetrics Face Recognition API, FaceR API by @Animetrics.

- Google: Google Cloud Vision API [-@Google]

- Microsoft: Microsoft Azure Cognitive Services Face API, published by @Microsoft.

- Skybiometry: @Skybiometry the spin off detection and recognition software of Neurotechnology. 

Data files provided are:

- ManualClassifiedFaces.csv resulted from the use of the Manual Annotation Application, it contains information about each face identified manually.
- ManualClassifiedScenes.csv also resulted from the use of the app, it contains information about each image.

# References

<div id="refs"></div>

